{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NVfTuEEzr-x"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import arviz as az\n",
    "import random\n",
    "from random import choice, choices, shuffle, sample\n",
    "from itertools import combinations, groupby\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from theano import shared\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "#pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "print('Running on PyMC3 v{}'.format(pm.__version__))\n",
    "import networkx as nx\n",
    "from IPython.display import display, HTML\n",
    "!pip install table-evaluator\n",
    "from table_evaluator import load_data, TableEvaluator\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQety2C_Rsje"
   },
   "source": [
    "# generate network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "io0aFE2aMeKs"
   },
   "source": [
    "#### small_sized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lPYj7oKoqwT"
   },
   "outputs": [],
   "source": [
    "def small_sized():\n",
    "\n",
    "  degreedist = random.choice(['HD','Norm'])\n",
    "  if degreedist=='HD':\n",
    "    high_hub=True\n",
    "    Norm_hub=False\n",
    "  else:\n",
    "    high_hub=False\n",
    "    Norm_hub=True\n",
    "\n",
    "  if high_hub:\n",
    "\n",
    "    while True:\n",
    "      # random pick network to generate from synthetic dataset\n",
    "      our_choice = random.choice(df2.index)\n",
    "      extreme_range = True\n",
    "\n",
    "      interval=df2.no_of_nodes[our_choice]*0.3\n",
    "      low=df2.no_of_nodes[our_choice]-interval\n",
    "      high=df2.no_of_nodes[our_choice]\n",
    "\n",
    "      #get similar degree distribution\n",
    "      df_degreedistribution=df[(low<=df.no_of_nodes) & (df.no_of_nodes<=high)]           \n",
    "\n",
    "      #get similar average degree \n",
    "      index_degree=[]\n",
    "      our_degree = df2.average_degree[our_choice]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (our_degree-2 > df_degreedistribution['average_degree'][index]) | ( df_degreedistribution['average_degree'][index] > our_degree+2):\n",
    "          index_degree.append(index)\n",
    "      df_degreedistribution = df_degreedistribution.drop(index_degree)\n",
    "\n",
    "      #filter based on high difference of max degree threshold\n",
    "      index_degree2=[]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (df_degreedistribution.no_of_nodes[index]-max(df_degreedistribution.degree_of_nodes[index]))/df_degreedistribution.no_of_nodes[index]>0.2:\n",
    "          index_degree2.append(index)\n",
    "      if len(index_degree2)>0:\n",
    "        break\n",
    "\n",
    "    all_degrees=[]\n",
    "    for indexx in index_degree2:\n",
    "      for degreee in df_degreedistribution.degree_of_nodes[indexx]:\n",
    "        all_degrees.append(degreee)\n",
    "\n",
    "    #define measurement based networks\n",
    "    change_range = True\n",
    "    edge_corners=df2.no_of_edges[our_choice]*2\n",
    "    syn_nodes=int(df2.no_of_nodes[our_choice])\n",
    "    cluster_coef = round(df2.average_clustering_coefficient[our_choice],2)\n",
    "    dia = df2.diameter[our_choice]\n",
    "\n",
    "    threshold=0\n",
    "    while True:\n",
    "      threshold+=1\n",
    "      if threshold>=100000:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      #constraint on average degree\n",
    "      if edge_corners-10<=sum(nodes_degree_list)<=edge_corners+10:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          #generate havel hakimi network from sampled degree distribution\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>5:\n",
    "            continue\n",
    "          else:\n",
    "            #reducing components\n",
    "            for f in range(len(components)-1):\n",
    "              for target in list(components[f+1]):\n",
    "                source = random.choice(list(components[0])[:5])\n",
    "                g.add_edge(source, target)     \n",
    "\n",
    "            G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "            #constraint on average clustering coefficient\n",
    "            if cluster_coef-0.01<=round((nx.average_clustering(G)),2)<=cluster_coef+0.01:\n",
    "              if nx.diameter(G)<=dia:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue \n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "    if extreme_range:\n",
    "      \n",
    "      interval=df2.no_of_nodes[our_choice]*0.3\n",
    "      low=df2.no_of_nodes[our_choice]-interval\n",
    "      high=df2.no_of_nodes[our_choice]\n",
    "\n",
    "      #get similar degree distribution\n",
    "      df_degreedistribution=df[(low<=df.no_of_nodes) & (df.no_of_nodes<=high)]           \n",
    "\n",
    "      #get similar average degree \n",
    "      index_degree=[]\n",
    "      our_degree = df2.average_degree[our_choice]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (our_degree-2 > df_degreedistribution['average_degree'][index]) | ( df_degreedistribution['average_degree'][index] > our_degree+2):\n",
    "          index_degree.append(index)\n",
    "      df_degreedistribution = df_degreedistribution.drop(index_degree)\n",
    "\n",
    "      #filter based on high difference of max degree threshold\n",
    "      index_degree2=[]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (df_degreedistribution.no_of_nodes[index]-max(df_degreedistribution.degree_of_nodes[index]))/df_degreedistribution.no_of_nodes[index]>0.1:\n",
    "          index_degree2.append(index)\n",
    "\n",
    "      all_degrees=[]\n",
    "      for indexx in index_degree2:\n",
    "        for degreee in df_degreedistribution.degree_of_nodes[indexx]:\n",
    "          all_degrees.append(degreee)\n",
    "\n",
    "      #define measurement based networks\n",
    "      edge_corners=df2.no_of_edges[our_choice]*2\n",
    "      syn_nodes=int(df2.no_of_nodes[our_choice])\n",
    "      cluster_coef = round(df2.average_clustering_coefficient[our_choice],2)\n",
    "      dia = df2.diameter[our_choice]\n",
    "\n",
    "      threshold=0\n",
    "      while extreme_range:\n",
    "        threshold+=1\n",
    "        if threshold>=100000:\n",
    "          Norm_hub=True\n",
    "          break\n",
    "        nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "        #constraint on average degree\n",
    "        if edge_corners-10<=sum(nodes_degree_list)<=edge_corners+10:\n",
    "          try:\n",
    "            sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "            #generate havel hakimi network from sampled degree distribution\n",
    "            g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "            components = dict(enumerate(nx.connected_components(g)))\n",
    "            if len(components)>8:\n",
    "              continue\n",
    "            else:\n",
    "              #reducing components\n",
    "              for f in range(len(components)-1):\n",
    "                for target in list(components[f+1]):\n",
    "                  source = random.choice(list(components[0])[:3])\n",
    "                  g.add_edge(source, target)     \n",
    "\n",
    "              G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "              #constraint on average clustering coefficient\n",
    "              if cluster_coef-0.1<=round((nx.average_clustering(G)),2)<=cluster_coef+0.1:\n",
    "                if nx.diameter(G)<=dia:\n",
    "                  change_range=False\n",
    "                  extreme_range=False\n",
    "                  break\n",
    "                else:\n",
    "                  continue \n",
    "              else:\n",
    "                continue\n",
    "          except:\n",
    "            continue      \n",
    "  \n",
    "\n",
    "  if Norm_hub:\n",
    " \n",
    "    while True:\n",
    "      # random pick network to generate from synthetic dataset\n",
    "      our_choice = random.choice(df2.index)\n",
    "      extreme_range = True\n",
    "\n",
    "      interval=df2.no_of_nodes[our_choice]*0.3\n",
    "      low=df2.no_of_nodes[our_choice]-interval\n",
    "      high=df2.no_of_nodes[our_choice]\n",
    "\n",
    "      #get similar degree distribution\n",
    "      df_degreedistribution=df[(low<=df.no_of_nodes) & (df.no_of_nodes<=high)]           \n",
    "\n",
    "      #get similar average degree \n",
    "      index_degree=[]\n",
    "      our_degree = df2.average_degree[our_choice]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (our_degree-2 > df_degreedistribution['average_degree'][index]) | ( df_degreedistribution['average_degree'][index] > our_degree+2):\n",
    "          index_degree.append(index)\n",
    "      df_degreedistribution = df_degreedistribution.drop(index_degree)\n",
    "\n",
    "      if len(df_degreedistribution.iloc[:,:4])>0:\n",
    "        break\n",
    "\n",
    "    all_degrees=[]\n",
    "    for indexx in df_degreedistribution.index:\n",
    "      for degreee in df_degreedistribution.degree_of_nodes[indexx]:\n",
    "        all_degrees.append(degreee)\n",
    "\n",
    "    #define measurement based networks\n",
    "    change_range = True\n",
    "    edge_corners=df2.no_of_edges[our_choice]*2\n",
    "    syn_nodes=int(df2.no_of_nodes[our_choice])\n",
    "    cluster_coef = round(df2.average_clustering_coefficient[our_choice],2)\n",
    "    dia = df2.diameter[our_choice]\n",
    "\n",
    "    threshold=0\n",
    "    while True:\n",
    "      threshold+=1\n",
    "      if threshold>=10000:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      #constraint on average degree\n",
    "      if edge_corners-10<=sum(nodes_degree_list)<=edge_corners+10:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          #generate havel hakimi network from sampled degree distribution\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>1:\n",
    "            continue\n",
    "          else:\n",
    "            G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "            #constraint on average clustering coefficient\n",
    "            if cluster_coef-0.01<=round((nx.average_clustering(G)),2)<=cluster_coef+0.01:\n",
    "              if (dia-1)<=nx.diameter(G)<=dia:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue \n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "    threshold=0\n",
    "    while change_range:\n",
    "      threshold+=1\n",
    "      if threshold>=10000:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      if edge_corners-30<=sum(nodes_degree_list)<=edge_corners+30:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True)\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>1:\n",
    "            continue\n",
    "          else:\n",
    "            G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "            if cluster_coef-0.02<=round((nx.average_clustering(G)),2)<=cluster_coef+0.02:\n",
    "              if (dia-1)<=nx.diameter(G)<=dia:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue\n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "    if extreme_range:\n",
    "      interval=df2.no_of_nodes[our_choice]*0.5\n",
    "      low=df2.no_of_nodes[our_choice]-interval\n",
    "      high=df2.no_of_nodes[our_choice]\n",
    "\n",
    "      df_degreedistribution=df[(low<=df.no_of_nodes) & (df.no_of_nodes<=high)]          \n",
    "\n",
    "      all_degrees=[]\n",
    "      for indexx in df_degreedistribution.index:\n",
    "        for degreee in df_degreedistribution.degree_of_nodes[indexx]:\n",
    "          all_degrees.append(degreee)\n",
    "\n",
    "    while extreme_range:\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      if edge_corners-40<=sum(nodes_degree_list)<=edge_corners+40:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>10:\n",
    "            continue\n",
    "          else:\n",
    "            #reducing components here\n",
    "            for f in range(len(components)-1):   \n",
    "              source = list(components[0])[0]\n",
    "              target = random.choice(list(components[f+1]))\n",
    "              g.add_edge(source, target)\n",
    "            G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "            if cluster_coef-0.3<=round((nx.average_clustering(G)),2)<=cluster_coef+0.3:\n",
    "              if nx.diameter(G)<=dia:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue\n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "  return G,df_degreedistribution,our_choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyHzWQQnV0IZ"
   },
   "source": [
    "#### medium_sized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3EafL3RYrtOL"
   },
   "outputs": [],
   "source": [
    "def medium_sized():\n",
    "\n",
    "  degreedist = random.choice(['HD','Norm'])\n",
    "  if degreedist=='HD':\n",
    "    high_hub=True\n",
    "    Norm_hub=False\n",
    "  else:\n",
    "    high_hub=False\n",
    "    Norm_hub=True\n",
    "\n",
    "  if high_hub:\n",
    "\n",
    "    while True:\n",
    "      # random pick network to generate from synthetic dataset\n",
    "      our_choice = random.choice(df2.index)\n",
    "      extreme_range = True\n",
    "\n",
    "      interval=df2.no_of_nodes[our_choice]*0.3\n",
    "      low=df2.no_of_nodes[our_choice]-interval\n",
    "      high=df2.no_of_nodes[our_choice]\n",
    "\n",
    "      #get similar degree distribution\n",
    "      df_degreedistribution=df[(low<=df.no_of_nodes) & (df.no_of_nodes<=high)]           \n",
    "\n",
    "      #get similar average degree \n",
    "      index_degree=[]\n",
    "      our_degree = df2.average_degree[our_choice]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (our_degree-2 > df_degreedistribution['average_degree'][index]) | ( df_degreedistribution['average_degree'][index] > our_degree+2):\n",
    "          index_degree.append(index)\n",
    "      df_degreedistribution = df_degreedistribution.drop(index_degree)\n",
    "\n",
    "      #filter based on high difference of max degree threshold\n",
    "      index_degree2=[]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (df_degreedistribution.no_of_nodes[index]-max(df_degreedistribution.degree_of_nodes[index]))/df_degreedistribution.no_of_nodes[index]>0.2:\n",
    "          index_degree2.append(index)\n",
    "      if len(index_degree2)>0:\n",
    "        break\n",
    "\n",
    "    all_degrees=[]\n",
    "    for indexx in index_degree2:\n",
    "      for degreee in df_degreedistribution.degree_of_nodes[indexx]:\n",
    "        all_degrees.append(degreee)\n",
    "\n",
    "    #define measurement based networks\n",
    "    change_range = True\n",
    "    edge_corners=df2.no_of_edges[our_choice]*2\n",
    "    syn_nodes=int(df2.no_of_nodes[our_choice])\n",
    "    cluster_coef = round(df2.average_clustering_coefficient[our_choice],2)\n",
    "    dia = df2.diameter[our_choice]\n",
    "\n",
    "    threshold=0\n",
    "    while True:\n",
    "      threshold+=1\n",
    "      if threshold>=100000:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      #constraint on average degree\n",
    "      if edge_corners-20<=sum(nodes_degree_list)<=edge_corners+20:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          #generate havel hakimi network from sampled degree distribution\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>10:\n",
    "            continue\n",
    "          else:\n",
    "            #reducing components\n",
    "            for f in range(len(components)-1):\n",
    "              for target in list(components[f+1]):\n",
    "                source = random.choice(list(components[0])[:5])\n",
    "                g.add_edge(source, target)     \n",
    "\n",
    "            G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "            #constraint on average clustering coefficient\n",
    "            if cluster_coef-0.01<=round((nx.average_clustering(G)),2)<=cluster_coef+0.01:\n",
    "              if nx.diameter(G)<=dia:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue \n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "    threshold=0\n",
    "    while change_range:\n",
    "      threshold+=1\n",
    "      if threshold>=10000:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      #constraint on average degree\n",
    "      if edge_corners-20<=sum(nodes_degree_list)<=edge_corners+20:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          #generate havel hakimi network from sampled degree distribution\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>20:\n",
    "            continue\n",
    "          else:\n",
    "            #reducing components\n",
    "            for f in range(len(components)-1):\n",
    "              for target in list(components[f+1]):\n",
    "                source = random.choice(list(components[0])[:5])\n",
    "                g.add_edge(source, target)     \n",
    "\n",
    "            G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "            #constraint on average clustering coefficient\n",
    "            if cluster_coef-0.1<=round((nx.average_clustering(G)),2)<=cluster_coef+0.1:\n",
    "              if nx.diameter(G)<=dia:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue \n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "    threshold=0\n",
    "    while extreme_range:\n",
    "      threshold+=1\n",
    "      if threshold>=100000:\n",
    "        Norm_hub=True\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      #constraint on average degree\n",
    "      if edge_corners-20<=sum(nodes_degree_list)<=edge_corners+20:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          #generate havel hakimi network from sampled degree distribution\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>20:\n",
    "            continue\n",
    "          else:\n",
    "            #reducing components\n",
    "            for f in range(len(components)-1):\n",
    "              for target in list(components[f+1]):\n",
    "                source = random.choice(list(components[0])[:5])\n",
    "                g.add_edge(source, target)     \n",
    "\n",
    "            G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "            #constraint on average clustering coefficient\n",
    "            if cluster_coef-0.2<=round((nx.average_clustering(G)),2)<=cluster_coef+0.2:\n",
    "              if nx.diameter(G)<=dia+1:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue \n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "  if Norm_hub:\n",
    "    while True:\n",
    "      # random pick network to generate from synthetic dataset  \n",
    "      our_choice = random.choice(df2.index)\n",
    "      extreme_range = True\n",
    "\n",
    "      interval=df2.no_of_nodes[our_choice]*0.3\n",
    "      low=df2.no_of_nodes[our_choice]-interval\n",
    "      high=df2.no_of_nodes[our_choice]\n",
    "\n",
    "      #get similar degree distribution\n",
    "      df_degreedistribution=df[(low<=df.no_of_nodes) & (df.no_of_nodes<=high)]          \n",
    "\n",
    "      #get similar average degree \n",
    "      index_degree=[]\n",
    "      our_degree = df2.average_degree[our_choice]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (our_degree-2 > df_degreedistribution['average_degree'][index]) | ( df_degreedistribution['average_degree'][index] > our_degree+2):\n",
    "          index_degree.append(index)\n",
    "      df_degreedistribution = df_degreedistribution.drop(index_degree)\n",
    "\n",
    "      if len(df_degreedistribution.iloc[:,:4])>0:\n",
    "        break\n",
    "\n",
    "    all_degrees=[]\n",
    "    for indexx in df_degreedistribution.index:\n",
    "      for degreee in df_degreedistribution.degree_of_nodes[indexx]:\n",
    "        all_degrees.append(degreee)\n",
    "  \n",
    "    #define measurement based networks\n",
    "    change_range = True\n",
    "    edge_corners=df2.no_of_edges[our_choice]*2\n",
    "    syn_nodes=int(df2.no_of_nodes[our_choice])\n",
    "    cluster_coef = round(df2.average_clustering_coefficient[our_choice],2)\n",
    "    dia = df2.diameter[our_choice]\n",
    "\n",
    "    threshold=0\n",
    "    while True:\n",
    "      threshold+=1\n",
    "      if threshold>=10000:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "        \n",
    "      #constraint on average degree\n",
    "      if edge_corners-20<=sum(nodes_degree_list)<=edge_corners+20:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          #generate havel hakimi network from sampled degree distribution\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>10:\n",
    "            continue\n",
    "          else:\n",
    "            #reduce components\n",
    "            for f in range(len(components)-1):   \n",
    "              source = list(components[0])[0]\n",
    "              target = random.choice(list(components[f+1]))\n",
    "              g.add_edge(source, target)\n",
    "\n",
    "            G=g \n",
    "            #constraint on average clustering coefficient\n",
    "            if cluster_coef-0.01<=round((nx.average_clustering(G)),2)<=cluster_coef+0.01:\n",
    "              if (dia-1)<=nx.diameter(G)<=dia:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue\n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "    threshold=0\n",
    "    while change_range:\n",
    "      threshold+=1\n",
    "      if threshold>=10000:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      if edge_corners-30<=sum(nodes_degree_list)<=edge_corners+30:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>10:\n",
    "            continue\n",
    "          else:\n",
    "            #reduce components\n",
    "            for f in range(len(components)-1):   \n",
    "              source = list(components[0])[0]\n",
    "              target = random.choice(list(components[f+1]))\n",
    "              g.add_edge(source, target)\n",
    "            G=g \n",
    "            if cluster_coef-0.1<=round((nx.average_clustering(G)),2)<=cluster_coef+0.1:\n",
    "              if nx.diameter(G)<=dia:  \n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue\n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "    if extreme_range:\n",
    "      \n",
    "      interval=df2.no_of_nodes[our_choice]*0.3\n",
    "      low=df2.no_of_nodes[our_choice]-interval\n",
    "      high=df2.no_of_nodes[our_choice]\n",
    "\n",
    "      df_degreedistribution=df[(low<=df.no_of_nodes) & (df.no_of_nodes<=high)]           \n",
    "\n",
    "      all_degrees=[]\n",
    "      for indexx in df_degreedistribution.index:\n",
    "        for degreee in df_degreedistribution.degree_of_nodes[indexx]:\n",
    "          all_degrees.append(degreee)\n",
    "\n",
    "    while extreme_range:\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      if edge_corners-50<=sum(nodes_degree_list)<=edge_corners+50:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>30:\n",
    "            continue\n",
    "          else:\n",
    "            #reduce components\n",
    "            for f in range(len(components)-1):   \n",
    "              source = list(components[0])[0]\n",
    "              target = random.choice(list(components[f+1]))\n",
    "              g.add_edge(source, target)\n",
    "            G=g \n",
    "            if cluster_coef-0.3<=round((nx.average_clustering(G)),2)<=cluster_coef+0.3:\n",
    "              if nx.diameter(G)<=dia+5:   \n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue\n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "  return G,df_degreedistribution,our_choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZGSIVS9IreH"
   },
   "source": [
    "#### large_sized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AD2dGjO-4XOr"
   },
   "outputs": [],
   "source": [
    "def large_sized():\n",
    "\n",
    "  degreedist = np.random.choice(['HD','Norm'],p=[0.3,0.7])\n",
    "  if degreedist=='HD':\n",
    "    high_hub=True\n",
    "    Norm_hub=False\n",
    "  else:\n",
    "    high_hub=False\n",
    "    Norm_hub=True\n",
    "  \n",
    "  if high_hub:\n",
    "    while True:\n",
    "      # random pick network to generate from synthetic dataset\n",
    "      our_choice = random.choice(df2.index)\n",
    "      extreme_range = True\n",
    "\n",
    "      interval=df2.no_of_nodes[our_choice]*0.3\n",
    "      low=df2.no_of_nodes[our_choice]-interval\n",
    "      high=df2.no_of_nodes[our_choice]\n",
    "\n",
    "      #get similar degree distribution\n",
    "      df_degreedistribution=df[(low<=df.no_of_nodes) & (df.no_of_nodes<=high)]           \n",
    "\n",
    "      #get similar average degree \n",
    "      index_degree=[]\n",
    "      our_degree = df2.average_degree[our_choice]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (our_degree-2 > df_degreedistribution['average_degree'][index]) | ( df_degreedistribution['average_degree'][index] > our_degree+2):\n",
    "          index_degree.append(index)\n",
    "      df_degreedistribution = df_degreedistribution.drop(index_degree)\n",
    "\n",
    "      #filter based on high difference of max degree threshold\n",
    "      index_degree2=[]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (df_degreedistribution.no_of_nodes[index]-max(df_degreedistribution.degree_of_nodes[index]))/df_degreedistribution.no_of_nodes[index]>0.2:\n",
    "          index_degree2.append(index)\n",
    "      if len(index_degree2)>0:\n",
    "        break\n",
    "\n",
    "    all_degrees=[]\n",
    "    for indexx in index_degree2:\n",
    "      for degreee in df_degreedistribution.degree_of_nodes[indexx]:\n",
    "        all_degrees.append(degreee)\n",
    "\n",
    "    #define measurement based networks\n",
    "    change_range = True\n",
    "    edge_corners=df2.no_of_edges[our_choice]*2\n",
    "    syn_nodes=int(df2.no_of_nodes[our_choice])\n",
    "    cluster_coef = round(df2.average_clustering_coefficient[our_choice],2)\n",
    "    dia = df2.diameter[our_choice]\n",
    "\n",
    "    if syn_nodes>300:\n",
    "        thres1,thres2=10,1000\n",
    "    else:\n",
    "        thres1,thres2=10000,10000  \n",
    "    threshold=0\n",
    "    while True:\n",
    "      threshold+=1\n",
    "      if threshold>=thres1:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      #constraint on average degree\n",
    "      if edge_corners-syn_nodes<=sum(nodes_degree_list)<=edge_corners+syn_nodes:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          #generate havel hakimi network from sampled degree distribution\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>20:\n",
    "            continue\n",
    "          else:\n",
    "            #reducing components\n",
    "            for f in range(len(components)-1):\n",
    "              for target in list(components[f+1]):\n",
    "                source = random.choice(list(components[0])[:5])\n",
    "                g.add_edge(source, target)     \n",
    "\n",
    "            G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "            #constraint on average clustering coefficient\n",
    "            if cluster_coef-0.15<=round((nx.average_clustering(G)),2)<=cluster_coef+0.15:\n",
    "              if nx.diameter(G)<=dia:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue \n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "    threshold=0\n",
    "    while change_range:\n",
    "      threshold+=1\n",
    "      if threshold>=thres2:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      inter = 1.5*syn_nodes\n",
    "      #constraint on average degree\n",
    "      if edge_corners-inter<=sum(nodes_degree_list)<=edge_corners+inter:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          #generate havel hakimi network from sampled degree distribution\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>35:\n",
    "            continue\n",
    "          else:\n",
    "            #reducing components\n",
    "            for f in range(len(components)-1):\n",
    "              for target in list(components[f+1]):\n",
    "                source = random.choice(list(components[0])[:5])\n",
    "                g.add_edge(source, target)     \n",
    "\n",
    "            G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "            #constraint on average clustering coefficient\n",
    "            if cluster_coef-0.15<=round((nx.average_clustering(G)),2)<=cluster_coef+0.15:\n",
    "              if nx.diameter(G)<=dia:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue \n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "    threshold=0\n",
    "    while extreme_range:\n",
    "      threshold+=1\n",
    "      if threshold>=10000:\n",
    "        Norm_hub=True\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      inter = 1.5*syn_nodes\n",
    "      #constraint on average degree\n",
    "      if edge_corners-inter<=sum(nodes_degree_list)<=edge_corners+inter:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          #generate havel hakimi network from sampled degree distribution\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>50:\n",
    "            continue\n",
    "          else:\n",
    "            #reducing components\n",
    "            for f in range(len(components)-1):\n",
    "              for target in list(components[f+1]):\n",
    "                source = random.choice(list(components[0])[:5])\n",
    "                g.add_edge(source, target)     \n",
    "\n",
    "            G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "            #constraint on average clustering coefficient\n",
    "            if cluster_coef-0.2<=round((nx.average_clustering(G)),2)<=cluster_coef+0.2:\n",
    "              change_range=False\n",
    "              extreme_range=False\n",
    "              break\n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "  if Norm_hub:\n",
    "    while True:\n",
    "      # random pick network to generate from synthetic dataset\n",
    "      our_choice = random.choice(df2.index)\n",
    "         \n",
    "      extreme_range = True\n",
    "\n",
    "      interval=df2.no_of_nodes[our_choice]*0.3\n",
    "      low=df2.no_of_nodes[our_choice]-interval\n",
    "      high=df2.no_of_nodes[our_choice]\n",
    "\n",
    "      #get similar degree distribution\n",
    "      df_degreedistribution=df[(low<=df.no_of_nodes) & (df.no_of_nodes<=high)]           \n",
    "\n",
    "      #get similar average degree \n",
    "      index_degree=[]\n",
    "      our_degree = df2.average_degree[our_choice]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (our_degree-2 > df_degreedistribution['average_degree'][index]) | ( df_degreedistribution['average_degree'][index] > our_degree+2):\n",
    "          index_degree.append(index)\n",
    "      df_degreedistribution = df_degreedistribution.drop(index_degree)\n",
    "\n",
    "      if len(df_degreedistribution.iloc[:,:4])>0:\n",
    "        break\n",
    "\n",
    "    all_degrees=[]\n",
    "    for indexx in df_degreedistribution.index:\n",
    "      for degreee in df_degreedistribution.degree_of_nodes[indexx]:\n",
    "        all_degrees.append(degreee)\n",
    "\n",
    "    #define measurement based networks\n",
    "    change_range = True\n",
    "    edge_corners=df2.no_of_edges[our_choice]*2\n",
    "    syn_nodes=int(df2.no_of_nodes[our_choice])\n",
    "    cluster_coef = round(df2.average_clustering_coefficient[our_choice],2)\n",
    "    dia = df2.diameter[our_choice]\n",
    "\n",
    "    if syn_nodes>340:\n",
    "        thres1,thres2=1,1000\n",
    "    else:\n",
    "        thres1,thres2=1000,10000  \n",
    "    threshold=0\n",
    "    while True:\n",
    "      threshold+=1\n",
    "      if threshold>=thres1:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      #constraint on average degree\n",
    "      inter = syn_nodes\n",
    "      if edge_corners-inter<=sum(nodes_degree_list)<=edge_corners+inter:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          #generate havel hakimi network from sampled degree distribution\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "\n",
    "          if len(components)>50:\n",
    "            continue\n",
    "          else:\n",
    "            #reduce components\n",
    "            for f in range(len(components)-1):   \n",
    "              source = list(components[0])[0]\n",
    "              target = random.choice(list(components[f+1]))\n",
    "              g.add_edge(source, target)\n",
    "            G=g \n",
    "            #constraint on average clustering coefficient\n",
    "            if cluster_coef-0.15<=round((nx.average_clustering(G)),2)<=cluster_coef+0.15:\n",
    "              if nx.diameter(G)<=dia:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue\n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "    threshold=0\n",
    "    while change_range:\n",
    "      threshold+=1\n",
    "      if threshold>=thres2:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      inter = 1.5*syn_nodes\n",
    "      if edge_corners-inter<=sum(nodes_degree_list)<=edge_corners+inter:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          \n",
    "          if len(components)>50:\n",
    "            continue\n",
    "          else:\n",
    "            #reduce components\n",
    "            for f in range(len(components)-1):   \n",
    "              source = list(components[0])[0]\n",
    "              target = random.choice(list(components[f+1]))\n",
    "              g.add_edge(source, target)\n",
    "\n",
    "            G=g \n",
    "            if cluster_coef-0.2<=round((nx.average_clustering(G)),2)<=cluster_coef+0.2:\n",
    "              if nx.diameter(G)<=dia+5:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue\n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "\n",
    "    threshold=0     \n",
    "    while extreme_range:\n",
    "      threshold+=1\n",
    "      if threshold>=10000:\n",
    "        break    \n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      inter = 2*syn_nodes\n",
    "      if edge_corners-inter<=sum(nodes_degree_list)<=edge_corners+inter:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>50:\n",
    "            continue\n",
    "          else:\n",
    "            #reduce components\n",
    "            for f in range(len(components)-1):   \n",
    "              source = list(components[0])[0]\n",
    "              target = random.choice(list(components[f+1]))\n",
    "              g.add_edge(source, target)       \n",
    "\n",
    "            G=g \n",
    "            if cluster_coef-0.3<=round((nx.average_clustering(G)),2)<=cluster_coef+0.3:\n",
    "              change_range=False\n",
    "              extreme_range=False\n",
    "              break\n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "  return G,df_degreedistribution,our_choice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0NS_G5KR9Rm"
   },
   "source": [
    "#### generate network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYZlQSvuOZWd"
   },
   "outputs": [],
   "source": [
    "#load real dataset\n",
    "a_file = open(\"data/Network_Metrics_real_dataset.pkl\", \"rb\")\n",
    "data = pickle.load(a_file)\n",
    "data=data.drop(columns=['repo'])\n",
    "data=data.sort_values(by='no_of_nodes').reset_index(drop=True)\n",
    "df=data.iloc[:,:]\n",
    "\n",
    "# choose group from small_sized,medium_sized,large_sized\n",
    "choosen_group = random.choice(['small_sized','medium_sized','large_sized'])\n",
    "\n",
    "iterate_dic = {'small_sized':[0,16],'medium_sized':[16,43],'large_sized':[43,100]}\n",
    "aa=iterate_dic[choosen_group][0]\n",
    "bb=iterate_dic[choosen_group][1]\n",
    "\n",
    "#load synthetic dataset\n",
    "df2=pd.read_csv('data/Network_Metrics_synthetic_dataset.csv',index_col=0)\n",
    "df2=df2.sort_values(by='no_of_nodes').reset_index(drop=True)\n",
    "df2=df2.iloc[aa:bb,:].reset_index(drop=True)\n",
    "\n",
    "#generate topology as per size\n",
    "if choosen_group=='small_sized':\n",
    "  G,df_degreedistribution,our_choice=small_sized()\n",
    "elif choosen_group=='medium_sized':\n",
    "  G,df_degreedistribution,our_choice=medium_sized()\n",
    "elif choosen_group=='large_sized':\n",
    "  G,df_degreedistribution,our_choice=large_sized()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfq_YMV6pqrW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NQety2C_Rsje",
    "io0aFE2aMeKs",
    "JyHzWQQnV0IZ",
    "gZGSIVS9IreH",
    "E0NS_G5KR9Rm",
    "IVafATRdo-2G",
    "VHhrJw_M5J3y",
    "6W8Y6GS7oPHo",
    "HHZieR9ToPHv",
    "uLJYcQtnoPHv",
    "p9YNOX20oPHv",
    "rkAj8I5JoPHw",
    "kJraMZB2oPHw",
    "xnTTIQXqoPHw",
    "sC2Pj2C_oPHx",
    "sHySL-jW5Pm6",
    "GA9D16dH5Pm6",
    "Jh3P0F0I5Pm7",
    "Y-RMel4l5Pm8",
    "UIwlWBNu5Pm8"
   ],
   "name": "Synthetic Topology Generation(updated) github",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
