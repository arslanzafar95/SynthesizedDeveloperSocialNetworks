{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: table-evaluator in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (1.2.2.post1)\n",
      "Requirement already satisfied: matplotlib in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from table-evaluator) (3.3.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from table-evaluator) (0.24.2)\n",
      "Requirement already satisfied: psutil in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from table-evaluator) (5.7.0)\n",
      "Requirement already satisfied: dython==0.5.1 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from table-evaluator) (0.5.1)\n",
      "Requirement already satisfied: tqdm in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from table-evaluator) (4.47.0)\n",
      "Requirement already satisfied: numpy in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from table-evaluator) (1.19.5)\n",
      "Requirement already satisfied: seaborn in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from table-evaluator) (0.11.1)\n",
      "Requirement already satisfied: pandas in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from table-evaluator) (1.1.4)\n",
      "Requirement already satisfied: scipy in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from table-evaluator) (1.5.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->table-evaluator) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->table-evaluator) (7.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->table-evaluator) (2.8.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->table-evaluator) (2020.6.20)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->table-evaluator) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->table-evaluator) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->table-evaluator) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->table-evaluator) (0.16.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from pandas->table-evaluator) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib->table-evaluator) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# all imports\n",
    "import base64\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "from networkx import Graph, DiGraph, simple_cycles\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures,StandardScaler,PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split,KFold,RepeatedKFold\n",
    "from sklearn.metrics import r2_score,mean_absolute_percentage_error\n",
    "from scipy.stats import spearmanr\n",
    "from numpy import mean,std\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from github import Github\n",
    "from tabulate import tabulate\n",
    "import random\n",
    "from random import choice, choices, shuffle, sample\n",
    "#pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "import networkx as nx\n",
    "!pip install table-evaluator\n",
    "from table_evaluator import load_data, TableEvaluator\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dic={'Android-Universal-Image-Loader':'nostra13/Android-Universal-Image-Loader',\n",
    "         'antlr':'antlr/antlr4',\n",
    "         'BroadleafCommerce':'BroadleafCommerce/BroadleafCommerce',\n",
    "         'hazelcast':'hazelcast/hazelcast',\n",
    "         'junit':'junit-team/junit',\n",
    "         'mapdb':'jankotek/mapdb',\n",
    "         'mcMMO':'mcMMO-Dev/mcMMO',\n",
    "         'nasa_mct':'nasa/mct',\n",
    "         'neo4j':'neo4j/neo4j',\n",
    "         'netty':'netty/netty',\n",
    "         'orientdb':'orientechnologies/orientdb',\n",
    "         'titan':'thinkaurelius/titan'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in repo_dic.items():\n",
    "\n",
    "    df=pd.read_csv('data/commits/'+key+'.csv',index_col=0)\n",
    "    \n",
    "    #remove committers which is null values \n",
    "    df=df.dropna(subset=['committer']).reset_index(drop=True)\n",
    "\n",
    "    lst=df.SHA.unique().tolist()\n",
    "\n",
    "    #remove files changes greater than 200\n",
    "    df=df[df['files changed']<200].reset_index(drop=True) \n",
    "\n",
    "    #group commits based on 3 months\n",
    "    df['date'] = df['date'].apply(lambda _: datetime.datetime.strptime(_,'%Y-%m-%d %H:%M:%S')) \n",
    "    df=df.sort_values(by='date',ascending=False).reset_index(drop=True)\n",
    "    #bug dataset collection end date\n",
    "    start_time = '6/1/15'     \n",
    "    # Define dates as datetime objects\n",
    "    start_time = datetime.datetime.strptime(start_time, '%m/%d/%y')\n",
    "    #start_time\n",
    "    time_change = datetime.timedelta(weeks=13)\n",
    "    #time_change\n",
    "    time_lst=[]\n",
    "    time=start_time\n",
    "    time_lst.append(time)\n",
    "    for s_date in df.date:\n",
    "        if time<df.iloc[-1].date:\n",
    "            break\n",
    "        time = time-time_change\n",
    "        time_lst.append(time)\n",
    " \n",
    "    #create commit groups\n",
    "    commitgroups=[]\n",
    "    for value in range(len(time_lst)-1):\n",
    "        df_interval=df [ (df.date<time_lst[value]) & (df.date>time_lst[value+1]) ]\n",
    "        commitgroups.extend(df_interval.groupby('filename')['committer'].unique().tolist())\n",
    "\n",
    "    #construct developer networks\n",
    "    %matplotlib inline\n",
    "    G_symmetric1 = nx.Graph()\n",
    "    for group in commitgroups:\n",
    "        test_list = group\n",
    "        res = [(a, b) for idx, a in enumerate(test_list) for b in test_list[idx + 1:]] \n",
    "        for x in res:    \n",
    "            G_symmetric1.add_edge(x[0],x[1])\n",
    "\n",
    "    #save commit groups characterized to DSN\n",
    "    a_file = open(\"data/usecase_data/commit_groups/commitGroups_\"+key+\".pkl\", \"wb\")\n",
    "    pickle.dump(commitgroups, a_file)\n",
    "    a_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in repo_dic.items():\n",
    "    \n",
    "    df=pd.read_csv('data/commits/'+key+'.csv',index_col=0)\n",
    "\n",
    "    a_file = open(\"data/usecase_data/commit_groups/commitGroups_\"+key+\".pkl\", \"rb\")\n",
    "    commit_groups = pickle.load(a_file)\n",
    "\n",
    "    # relevant repository bugdataset \n",
    "    df_bugdata=pd.read_csv('data/usecase_data/bug_data/'+key+'_BugData.csv',index_col=0)\n",
    "    df_bugdata=df_bugdata[['LongName','Number of bugs']]\n",
    "    df_bugdata.columns=['filename','no_of_bugs']\n",
    "    df_bugdata=df_bugdata.groupby(by='filename').sum().reset_index()\n",
    "\n",
    "    #preprocessing df\n",
    "    df['date'] = df['date'].apply(lambda _: datetime.datetime.strptime(_,'%Y-%m-%d %H:%M:%S'))\n",
    "    df=df.sort_values(by='date',ascending=False).reset_index(drop=True)\n",
    "    #remove files after bug dataset end date 6/1/15\n",
    "    start_time = '6/1/15'     \n",
    "    # Define dates as datetime objects\n",
    "    start_time = datetime.datetime.strptime(start_time, '%m/%d/%y')\n",
    "    df = df [(df.date<start_time)]\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    #Dataframe for committers groups for files\n",
    "    df_file_committers=pd.DataFrame(df.groupby('filename')['committer'].unique()).reset_index()\n",
    "\n",
    "    # define graph\n",
    "    %matplotlib inline\n",
    "    G_symmetric = nx.Graph()\n",
    "    for group in commit_groups:\n",
    "        test_list = group\n",
    "        res = [(a, b) for idx, a in enumerate(test_list) for b in test_list[idx + 1:]] \n",
    "        for x in res:    \n",
    "            G_symmetric.add_edge(x[0],x[1])\n",
    "    \n",
    "    #find file-based network centrality metrics\n",
    "    df_file_committers['degree_sum']=0.0\n",
    "    df_file_committers['betweenness_sum']=0.0\n",
    "    df_file_committers['closeness_sum']=0.0\n",
    "    df_file_committers['clustering_sum']=0.0\n",
    "    df_file_committers['eigenvector_sum']=0.0\n",
    "    df_file_committers['pagerank_sum']=0.0\n",
    "    df_file_committers['hubs_sum']=0.0\n",
    "    df_file_committers['Number of Hubs']=0\n",
    "    \n",
    "    hub_limit=int(sorted(dict(nx.degree(G_symmetric)).values(),reverse=True)[0]*0.5) \n",
    "    hubs_dict=nx.hits(G_symmetric)[0]\n",
    "    pagerank_dict=nx.pagerank(G_symmetric)\n",
    "    eigenvector_dict=nx.eigenvector_centrality(G_symmetric)\n",
    "    clustering_dict=nx.clustering(G_symmetric)\n",
    "    betweenness_dict=nx.betweenness_centrality(G_symmetric)\n",
    "    closeness_dict=nx.closeness_centrality(G_symmetric)\n",
    "    \n",
    "    for index in df_file_committers.index:\n",
    "        degree_sum=0.0\n",
    "        betweenness_sum=0.0\n",
    "        closeness_sum=0.0\n",
    "        clustering_sum=0.0\n",
    "        eigenvector_sum=0.0\n",
    "        pagerank_sum=0.0\n",
    "        hubs_sum=0.0\n",
    "        count_hubs=0\n",
    "        for author in df_file_committers['committer'][index]:\n",
    "            try:    \n",
    "                degree_sum+=G_symmetric.degree(author)\n",
    "                betweenness_sum+=betweenness_dict[author]\n",
    "                closeness_sum+=closeness_dict[author] \n",
    "                clustering_sum+=clustering_dict[author]\n",
    "                eigenvector_sum+=eigenvector_dict[author]\n",
    "                pagerank_sum+=pagerank_dict[author]\n",
    "                hubs_sum+=hubs_dict[author]\n",
    "                if G_symmetric.degree(author)>hub_limit:\n",
    "                    count_hubs+=1\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        # take average of all centrality measures\n",
    "        df_file_committers['degree_sum'][index]=degree_sum/len(df_file_committers['committer'][index])\n",
    "        df_file_committers['betweenness_sum'][index]=betweenness_sum/len(df_file_committers['committer'][index])\n",
    "        df_file_committers['closeness_sum'][index]=closeness_sum/len(df_file_committers['committer'][index])\n",
    "        df_file_committers['clustering_sum'][index]=clustering_sum/len(df_file_committers['committer'][index])\n",
    "        df_file_committers['eigenvector_sum'][index]=eigenvector_sum/len(df_file_committers['committer'][index])\n",
    "        df_file_committers['pagerank_sum'][index]=pagerank_sum/len(df_file_committers['committer'][index])\n",
    "        df_file_committers['hubs_sum'][index]=hubs_sum/len(df_file_committers['committer'][index])\n",
    "        df_file_committers['Number of Hubs'][index]=count_hubs\n",
    "    \n",
    "    #dataframe for number of times each file is touched\n",
    "    df_file_updates=pd.DataFrame(df['filename'].value_counts()).reset_index().rename(columns={'filename':'Updates','index':'filename'})\n",
    "\n",
    "    #Dataframe for network metrics\n",
    "    df_centrality_measures=pd.merge(df_file_committers,df_file_updates,right_on='filename',left_on='filename')\n",
    "    df_centrality_measures['Developers']=df_centrality_measures['committer'].apply(lambda x: len(x))\n",
    "    \n",
    "    #Dataframe for code churn in each file\n",
    "    df_churn=pd.DataFrame(df.groupby('filename')['file changes'].sum()).reset_index().rename(columns={'file changes':'code churn'})\n",
    "\n",
    "    #Dataframe including code churn metric\n",
    "    df_centrality_churn=pd.merge(df_centrality_measures,df_churn,right_on='filename',left_on='filename')\n",
    "\n",
    "    #training dataframe with Bug data included\n",
    "    df_trainData=pd.merge(df_centrality_churn,df_bugdata,right_on='filename',left_on='filename') \n",
    "\n",
    "    #save training data\n",
    "    df_trainData.to_csv('data/usecase_data/train_data/'+key+'_TrainData.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data for different projects\n",
    "df1=pd.read_csv('data/usecase_data/train_data/Android-Universal-Image-Loader_TrainData.csv',index_col=0)\n",
    "df2=pd.read_csv('data/usecase_data/train_data/antlr_TrainData.csv',index_col=0)\n",
    "df3=pd.read_csv('data/usecase_data/train_data/BroadleafCommerce_TrainData.csv',index_col=0)\n",
    "df5=pd.read_csv('data/usecase_data/train_data/hazelcast_TrainData.csv',index_col=0)\n",
    "df14=pd.read_csv('data/usecase_data/train_data/junit_TrainData.csv',index_col=0)\n",
    "df7=pd.read_csv('data/usecase_data/train_data/mapdb_TrainData.csv',index_col=0)\n",
    "df8=pd.read_csv('data/usecase_data/train_data/mcMMO_TrainData.csv',index_col=0)\n",
    "df9=pd.read_csv('data/usecase_data/train_data/nasa_mct_TrainData.csv',index_col=0)\n",
    "df10=pd.read_csv('data/usecase_data/train_data/neo4j_TrainData.csv',index_col=0)\n",
    "df11=pd.read_csv('data/usecase_data/train_data/netty_TrainData.csv',index_col=0)\n",
    "df12=pd.read_csv('data/usecase_data/train_data/orientdb_TrainData.csv',index_col=0)\n",
    "df13=pd.read_csv('data/usecase_data/train_data/titan_TrainData.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non average centrality measures\n",
    "data_lst = [df1,df2,df3,df5,df14,df7,df8,df9,df10,df11,df12,df13] \n",
    "for data in data_lst:\n",
    "    data['degree_sum']=data['degree_sum']*data['Developers']\n",
    "    data['betweenness_sum']=data['betweenness_sum']*data['Developers']\n",
    "    data['closeness_sum']=data['closeness_sum']*data['Developers']\n",
    "    data['clustering_sum']=data['clustering_sum']*data['Developers']\n",
    "    data['eigenvector_sum']=data['eigenvector_sum']*data['Developers']\n",
    "    data['pagerank_sum']=data['pagerank_sum']*data['Developers']\n",
    "    data['hubs_sum']=data['hubs_sum']*data['Developers']\n",
    "    data['len_developers']=data['Developers']\n",
    "    #drop zero rows\n",
    "    data.drop(data[(data.degree_sum==0) & (data.betweenness_sum==0) & (data.closeness_sum==0)].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize predictor metrics\n",
    "cols_to_norm = ['degree_sum','betweenness_sum','closeness_sum','Developers','clustering_sum', 'eigenvector_sum', \n",
    "                'pagerank_sum','hubs_sum','Number of Hubs']\n",
    "data_lst = [df1,df2,df3,df5,df14,df7,df8,df9,df10,df11,df12,df13] \n",
    "for data in data_lst:\n",
    "    data[cols_to_norm] = data[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "#find probability of bug occurrence\n",
    "cols_to_norm = ['no_of_bugs']\n",
    "data_lst = [df1,df2,df3,df5,df14,df7,df8,df9,df10,df11,df12,df13] \n",
    "for data in data_lst:\n",
    "    data[cols_to_norm] = data[cols_to_norm].apply(lambda x: (x) / (x.sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge data\n",
    "df=df1.append([df2,df3,df5,df14,df7,df8,df9,df10,df11,df12,df13]) \n",
    "#select required columns \n",
    "df=df.reset_index(drop=True)\n",
    "df=df[['degree_sum','betweenness_sum','closeness_sum','Developers','clustering_sum', 'eigenvector_sum', \n",
    "                'pagerank_sum','hubs_sum','Number of Hubs','no_of_bugs','committer','len_developers','filename']]\n",
    "\n",
    "df.rename(columns={'no_of_bugs':'bug_probability'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#undersample non-buggy data\n",
    "df_zero = df[df.bug_probability==0]\n",
    "df_nonzero = df[df.bug_probability!=0]\n",
    "\n",
    "#drop duplicates\n",
    "df_zero.drop_duplicates(inplace=True)\n",
    "df_nonzero.drop_duplicates(inplace=True)\n",
    "\n",
    "perc = df_nonzero.shape[0]/df_zero.shape[0]         \n",
    "df_zero_sampled=df_zero.sample(frac=perc,random_state=2550)  \n",
    "\n",
    "#equalized portions\n",
    "df_sampled = df_nonzero.append(df_zero_sampled).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates\n",
    "df_model=df_sampled       \n",
    "df_model.drop_duplicates(inplace=True)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transformations\n",
    "\n",
    "df_model.iloc[:,:-3]=df_model.iloc[:,:-3]+0.000001 \n",
    "\n",
    "df_model['degree_sum'] = np.log(df_model['degree_sum'])\n",
    "df_model['Developers'] = np.log(df_model['Developers'])\n",
    "df_model['closeness_sum'] = np.log(df_model['closeness_sum'])\n",
    "df_model['Number of Hubs'] = np.log(df_model['Number of Hubs'])\n",
    "df_model['betweenness_sum'] = np.log(df_model['betweenness_sum'])\n",
    "df_model['clustering_sum'] = np.log(df_model['clustering_sum'])\n",
    "df_model['eigenvector_sum'] = np.log(df_model['eigenvector_sum'])\n",
    "df_model['pagerank_sum'] = np.log(df_model['pagerank_sum'])\n",
    "df_model['hubs_sum'] = np.log(df_model['hubs_sum'])\n",
    "df_model['bug_probability'] = np.log(df_model['bug_probability'])\n",
    "\n",
    "# shuffle data\n",
    "df_model=df_model.sample(frac=1,random_state=2550)      \n",
    "#df_model.to_csv('data/usecase_data/df_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROSS VALIDATION\n",
    "\n",
    "n_splits=5\n",
    "kf5 = KFold(n_splits=n_splits) \n",
    "\n",
    "spearman_sum=0\n",
    "MAPE_sum=0\n",
    "\n",
    "for train_index, test_index in kf5.split(df_model):\n",
    "    X_train = df_model.iloc[train_index].iloc[:, [0,1,2,3,7]]\n",
    "    X_test = df_model.iloc[test_index].iloc[:, [0,1,2,3,7]]\n",
    "    y_train = df_model.iloc[train_index].loc[:,'bug_probability']\n",
    "    y_test =  df_model.iloc[test_index].loc[:,'bug_probability']\n",
    "\n",
    "    #Ridge regression\n",
    "    model =  Ridge()\n",
    "    #Training the model  \n",
    "    model.fit(X_train, y_train)   \n",
    "    predictions = model.predict(X_test)\n",
    "    # calculate spearman's correlation\n",
    "    coef, p = spearmanr(predictions, y_test)\n",
    "    spearman_sum+=coef\n",
    "    MAPE_sum+=mean_absolute_percentage_error(y_test, predictions)\n",
    "\n",
    "# model    \n",
    "X=df_model.iloc[:,[0,1,2,3,7]]\n",
    "y=df_model.iloc[:,[9]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=2550)  \n",
    "model = Ridge()                  \n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
