{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AB7iI3GQ0Oyz",
    "outputId": "68b0e9a4-5fce-4e35-9151-91a94c8f1407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on PyMC3 v3.11.2\n",
      "Requirement already satisfied: table-evaluator in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (1.2.2.post1)\n",
      "Requirement already satisfied: scipy in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from table-evaluator) (1.5.0)\n",
      "Requirement already satisfied: dython==0.5.1 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from table-evaluator) (0.5.1)\n",
      "Requirement already satisfied: matplotlib in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from table-evaluator) (3.3.2)\n",
      "Requirement already satisfied: psutil in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from table-evaluator) (5.7.0)\n",
      "Requirement already satisfied: pandas in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from table-evaluator) (1.1.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from table-evaluator) (0.24.2)\n",
      "Requirement already satisfied: seaborn in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from table-evaluator) (0.11.1)\n",
      "Requirement already satisfied: numpy in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from table-evaluator) (1.19.5)\n",
      "Requirement already satisfied: tqdm in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from table-evaluator) (4.47.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->table-evaluator) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->table-evaluator) (2020.6.20)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->table-evaluator) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->table-evaluator) (1.2.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->table-evaluator) (7.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->table-evaluator) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from pandas->table-evaluator) (2020.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->table-evaluator) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->table-evaluator) (0.16.0)\n",
      "Requirement already satisfied: six in /Users/arslanzafar/opt/anaconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->table-evaluator) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import arviz as az\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from theano import shared\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import PolynomialFeatures,StandardScaler,PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.model_selection import train_test_split,KFold,RepeatedKFold,StratifiedKFold\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error\n",
    "#pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "print('Running on PyMC3 v{}'.format(pm.__version__))\n",
    "import networkx as nx\n",
    "import random\n",
    "from scipy.stats import spearmanr\n",
    "from numpy import mean,std\n",
    "from random import choice, choices, shuffle, sample\n",
    "from itertools import combinations, groupby\n",
    "from IPython.display import display, HTML\n",
    "!pip install table-evaluator\n",
    "from table_evaluator import load_data, TableEvaluator\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "KeyE-KFRRnl3",
    "outputId": "d9e787df-45d6-4f10-ecf8-6c3609ac41a6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_nodes</th>\n",
       "      <th>no_of_edges</th>\n",
       "      <th>average_degree</th>\n",
       "      <th>average_clustering_coefficient</th>\n",
       "      <th>diameter</th>\n",
       "      <th>radius</th>\n",
       "      <th>powerlaw_exponent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>85</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.767825</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6.580027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>78</td>\n",
       "      <td>3.804878</td>\n",
       "      <td>0.738816</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3.816859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>123</td>\n",
       "      <td>5.857143</td>\n",
       "      <td>0.709830</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5.054764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>49</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0.250232</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.527568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>143</td>\n",
       "      <td>6.651163</td>\n",
       "      <td>0.687416</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4.182046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>349</td>\n",
       "      <td>2133</td>\n",
       "      <td>12.223496</td>\n",
       "      <td>0.756432</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2.310369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>363</td>\n",
       "      <td>4191</td>\n",
       "      <td>23.090909</td>\n",
       "      <td>0.752535</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2.142328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>377</td>\n",
       "      <td>5522</td>\n",
       "      <td>29.294430</td>\n",
       "      <td>0.727573</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5.255193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>387</td>\n",
       "      <td>6626</td>\n",
       "      <td>34.242894</td>\n",
       "      <td>0.711831</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2.212416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>420</td>\n",
       "      <td>1107</td>\n",
       "      <td>5.271429</td>\n",
       "      <td>0.583100</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2.733805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    no_of_nodes  no_of_edges  average_degree  average_clustering_coefficient  \\\n",
       "0            25           85        6.800000                        0.767825   \n",
       "1            41           78        3.804878                        0.738816   \n",
       "2            42          123        5.857143                        0.709830   \n",
       "3            42           49        2.333333                        0.250232   \n",
       "4            43          143        6.651163                        0.687416   \n",
       "..          ...          ...             ...                             ...   \n",
       "72          349         2133       12.223496                        0.756432   \n",
       "73          363         4191       23.090909                        0.752535   \n",
       "74          377         5522       29.294430                        0.727573   \n",
       "75          387         6626       34.242894                        0.711831   \n",
       "76          420         1107        5.271429                        0.583100   \n",
       "\n",
       "    diameter  radius  powerlaw_exponent  \n",
       "0          3       2           6.580027  \n",
       "1          3       2           3.816859  \n",
       "2          3       2           5.054764  \n",
       "3          2       1           3.527568  \n",
       "4          4       2           4.182046  \n",
       "..       ...     ...                ...  \n",
       "72         5       3           2.310369  \n",
       "73         4       3           2.142328  \n",
       "74         6       3           5.255193  \n",
       "75         4       3           2.212416  \n",
       "76         5       3           2.733805  \n",
       "\n",
       "[77 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_file = open(\"scripts/data/Network_Metrics_real_dataset.pkl\", \"rb\")\n",
    "data = pickle.load(a_file)\n",
    "data=data.drop(columns=['repo'])\n",
    "df=data.iloc[:,[0,1,2,3,7,8,19]]\n",
    "df=df.sort_values(by='no_of_nodes').reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2yI-4y4lEVhn"
   },
   "outputs": [],
   "source": [
    "df=df.sort_values(by='average_degree').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjI_6NL50Oy5"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhkOQfndEVhd"
   },
   "source": [
    "# Data Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6birH6XB0Oy6"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "#find parameters of beta distribution\n",
    "a,b, loc,scale = stats.beta.fit(df.no_of_nodes)\n",
    "# sample values from fitted distribution\n",
    "node_samples = beta.rvs(a,b, loc,scale, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yCXLZgJz0Oy7"
   },
   "outputs": [],
   "source": [
    "# log transformation\n",
    "df=np.log10(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "id": "Q1oAd7gqLSPt",
    "outputId": "d2238513-76fb-4516-e5ec-eab09df096dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-b2c16ba2bdf9>:24: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  regression_trace_nodes_edges = pm.sample(1000, step, chains=1)\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "NUTS: [epsilon, zeta, delta, gamma, beta, alpha]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2000' class='' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [2000/2000 01:55<00:00 Sampling chain 0, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 1 chain for 1_000 tune and 1_000 draw iterations (1_000 + 1_000 draws total) took 115 seconds.\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n"
     ]
    }
   ],
   "source": [
    "# bayesian model edges\n",
    "\n",
    "edges_values = df['no_of_edges'].values.reshape(-1, 1)\n",
    "\n",
    "with pm.Model() as regression_model_nodes_edges:\n",
    "    \n",
    "    nodes_values = pm.Data(\"nodes_values\", df['no_of_nodes'].values.reshape(-1, 1))\n",
    "\n",
    "    alpha = pm.Normal('alpha', mu = 15.13437422, sd =.5)\n",
    "  \n",
    "    beta = pm.Normal('beta', mu = -28.74787971, sd = .5)\n",
    "    gamma = pm.Normal('gamma', mu = 22.3704513, sd = .5)\n",
    "    delta = pm.Normal('delta', mu = -7.47195216, sd = .5)\n",
    "    zeta = pm.Normal('zeta', mu = 0.94787594, sd = .5)\n",
    "\n",
    "    epsilon = pm.HalfNormal('epsilon', sd = .01)\n",
    "    \n",
    "    by_mean = alpha + beta * nodes_values + gamma * nodes_values**2 + delta * nodes_values**3 + zeta * nodes_values**4\n",
    "    \n",
    "    Ylikelihood = pm.Normal('Ylikelihood', mu = by_mean, sd = epsilon, observed = edges_values)\n",
    "        \n",
    "    step = pm.NUTS()\n",
    "\n",
    "    regression_trace_nodes_edges = pm.sample(1000, step, chains=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "Dy8le0lS8l_6",
    "outputId": "33b45093-6610-4941-cba4-55d94f5f7720"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1000' class='' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1000/1000 00:04<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# log transformation of sampled nodes \n",
    "node_samples_transformed=np.log10(node_samples)\n",
    "with regression_model_nodes_edges:\n",
    "    pm.set_data({\"nodes_values\": np.array(node_samples_transformed).reshape(-1, 1)})\n",
    "    posterior_predictive_nodes_edges = pm.sample_posterior_predictive(regression_trace_nodes_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SO6TGQOzmkZk"
   },
   "outputs": [],
   "source": [
    "samples_nodes_edges=posterior_predictive_nodes_edges  \n",
    "\n",
    "# select from different models\n",
    "sample_no=random.choices(range(1000),k=100) \n",
    "samples_lst=[]\n",
    "for x in range(100): \n",
    "  model_number = sample_no[x]\n",
    "  model_value = samples_nodes_edges['Ylikelihood'][model_number][x]\n",
    "  samples_lst.append(model_value[0])\n",
    "\n",
    "full_prior_samples = pd.DataFrame(samples_lst, columns=['no_of_edges'])\n",
    "full_prior_samples['no_of_nodes']=np.array(node_samples_transformed)\n",
    "full_prior_samples=full_prior_samples.drop_duplicates().sort_values(by='no_of_nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJMtr28CEVhq"
   },
   "outputs": [],
   "source": [
    "df_synthetic=full_prior_samples[['no_of_nodes','no_of_edges']]\n",
    "df_synthetic=10**df_synthetic\n",
    "df_synthetic['average_degree']=(df_synthetic['no_of_edges']*2)/(df_synthetic['no_of_nodes'])\n",
    "df_synthetic=np.log10(df_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "id": "SBvK3Bl6OlXs",
    "outputId": "c43525c2-8635-442b-e9e0-d48f0d8bf952"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "NUTS: [epsilon, zeta, delta, gamma, beta, alpha]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2000' class='' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [2000/2000 02:02<00:00 Sampling chain 0, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 1 chain for 1_000 tune and 1_000 draw iterations (1_000 + 1_000 draws total) took 122 seconds.\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n"
     ]
    }
   ],
   "source": [
    "# bayesian model CC\n",
    "\n",
    "avg_clustering_values = df['average_clustering_coefficient'].values.reshape(-1, 1)\n",
    "\n",
    "with pm.Model() as regression_model_degree_clustering:\n",
    "\n",
    "    avg_degree_values = pm.Data(\"avg_degree_values\", df['average_degree'].values.reshape(-1, 1))  \n",
    "\n",
    "    alpha = pm.Normal('alpha', mu = -2.57778591, sd =.5)\n",
    "    \n",
    "    beta = pm.Normal('beta', mu = 8.67701697, sd = .5)\n",
    "    gamma = pm.Normal('gamma', mu = -11.64685216, sd = .5)\n",
    "    delta = pm.Normal('delta', mu = 6.97481184, sd = .5)\n",
    "    zeta = pm.Normal('zeta', mu = -1.55689117, sd = .5)\n",
    "    \n",
    "    epsilon = pm.HalfNormal('epsilon', sd = 0.001)\n",
    "    \n",
    "    by_mean = alpha + beta * avg_degree_values + gamma * avg_degree_values**2 + delta * avg_degree_values**3 + zeta * avg_degree_values**4 \n",
    "    \n",
    "    Ylikelihood = pm.Normal('Ylikelihood', mu = by_mean, sd = epsilon, observed = avg_clustering_values)\n",
    "    \n",
    "    step = pm.NUTS()\n",
    "    \n",
    "    regression_trace_degree_clustering = pm.sample(1000, step, chains=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "FpGWp4VYkF9d",
    "outputId": "46fc987e-47fd-4b58-ed0a-da5f27969e5e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1000' class='' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1000/1000 00:03<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with regression_model_degree_clustering:\n",
    "    pm.set_data({\"avg_degree_values\": df_synthetic['average_degree'].values.reshape(-1, 1)})\n",
    "    posterior_predictive_degree_clustering = pm.sample_posterior_predictive(regression_trace_degree_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nR0Tkk4rlt-"
   },
   "outputs": [],
   "source": [
    "samples_degree_clustering=posterior_predictive_degree_clustering\n",
    "\n",
    "# select from different models\n",
    "sample_no=random.choices(range(1000),k=100)\n",
    "samples_lst=[]\n",
    "for x in range(100):\n",
    "  model_number = sample_no[x]\n",
    "  model_value = samples_degree_clustering['Ylikelihood'][model_number][x]\n",
    "  samples_lst.append(model_value[0])\n",
    "\n",
    "full_prior_samples = pd.DataFrame(samples_lst, columns=['average_clustering_coefficient'])\n",
    "full_prior_samples['average_degree']=df_synthetic.average_degree.values   \n",
    "full_prior_samples=full_prior_samples.drop_duplicates().sort_values(by='average_degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GqCXul3QcbAd"
   },
   "outputs": [],
   "source": [
    "df_synthetic['average_clustering_coefficient']=full_prior_samples['average_clustering_coefficient']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaxw9NALck9q"
   },
   "outputs": [],
   "source": [
    "df_synthetic=10**df_synthetic\n",
    "df_synthetic.no_of_nodes=df_synthetic.no_of_nodes.round(0)\n",
    "df_synthetic.no_of_edges=df_synthetic.no_of_edges.round(0)\n",
    "df_synthetic=df_synthetic.sort_values(by='no_of_nodes').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39lEzHReTTbY"
   },
   "outputs": [],
   "source": [
    "df=data.iloc[:,[0,1,2,3,7,8,19]]\n",
    "df=df.sort_values(by='average_degree').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# powerlaw exponent synthesis\n",
    "from scipy.stats import exponnorm\n",
    "#fit\n",
    "a, loc,scale = stats.exponnorm.fit(df.powerlaw_exponent)\n",
    "# sample values\n",
    "samples_powerlaw = exponnorm.rvs(a, loc,scale, size=100)  \n",
    "df_synthetic['powerlaw_exponent'] = samples_powerlaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3GU4opTXWYE"
   },
   "outputs": [],
   "source": [
    "df_synthetic=df_synthetic.sort_values(by='no_of_nodes').reset_index(drop=True)\n",
    "df=df.sort_values(by='no_of_nodes').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOvG9xYYEWIg"
   },
   "outputs": [],
   "source": [
    "# diameter synthesis\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "def get_distribution(dist):\n",
    "  # choose distribution\n",
    "  if dist==1:\n",
    "    lowerBound=0;upperBound=30\n",
    "  elif dist==2:\n",
    "    lowerBound=30;upperBound=50\n",
    "  elif dist==3:\n",
    "    lowerBound=50;upperBound=77\n",
    "  dias=df.diameter[lowerBound:upperBound]\n",
    "  counts=dict(Counter(dias))\n",
    "  # find counts\n",
    "  probabs_abs=[]\n",
    "  lst=list(range(1,9))\n",
    "  for value in lst:\n",
    "    try:\n",
    "      probabs_abs.append(counts[value])\n",
    "    except:\n",
    "      probabs_abs.append(0)\n",
    "  # find probabilites\n",
    "  probabs=[float(i)/sum(probabs_abs) for i in probabs_abs]\n",
    "  discrete_dist = stats.rv_discrete(name='custm', values=(lst, probabs))\n",
    "  \n",
    "  return discrete_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFTecF2AZiQw"
   },
   "outputs": [],
   "source": [
    "# get length of each groups\n",
    "sample1_len = len(df_synthetic[df_synthetic.no_of_nodes<=df.no_of_nodes[30]])\n",
    "sample2_len = len(df_synthetic[(df_synthetic.no_of_nodes>df.no_of_nodes[30]) & (df_synthetic.no_of_nodes<=df.no_of_nodes[50])])\n",
    "sample3_len = len(df_synthetic[df_synthetic.no_of_nodes>df.no_of_nodes[50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_dia1=get_distribution(1).rvs(size=sample1_len)\n",
    "samples_dia2=get_distribution(2).rvs(size=sample2_len)\n",
    "samples_dia3=get_distribution(3).rvs(size=sample3_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwdsKLIoeJyc"
   },
   "outputs": [],
   "source": [
    "dias_lst=list(samples_dia1)\n",
    "dias_lst.extend(list(samples_dia2));dias_lst.extend(list(samples_dia3))\n",
    "#dias_lst\n",
    "df_synthetic['diameter']=dias_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QWVOMw0ZjKg"
   },
   "outputs": [],
   "source": [
    "# getting radius\n",
    "import math\n",
    "df_synthetic['radius']=df_synthetic['diameter'].apply(lambda x: math.ceil(x/2))\n",
    "rand_choice=random.choices([0,1], weights = [0.961,0.039], k = 100)        # weights based on given data \n",
    "df_synthetic['radius']=df_synthetic['radius']+rand_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_nodes</th>\n",
       "      <th>no_of_edges</th>\n",
       "      <th>average_degree</th>\n",
       "      <th>average_clustering_coefficient</th>\n",
       "      <th>diameter</th>\n",
       "      <th>radius</th>\n",
       "      <th>powerlaw_exponent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>6.238131</td>\n",
       "      <td>0.703321</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.407586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>5.260353</td>\n",
       "      <td>0.633320</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.984418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>5.349207</td>\n",
       "      <td>0.683539</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.365894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>6.456137</td>\n",
       "      <td>0.732987</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.362633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>8.758000</td>\n",
       "      <td>0.713908</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3.267557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>376.0</td>\n",
       "      <td>6951.0</td>\n",
       "      <td>36.987107</td>\n",
       "      <td>0.766860</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2.778269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>380.0</td>\n",
       "      <td>5278.0</td>\n",
       "      <td>27.781883</td>\n",
       "      <td>0.768408</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.982870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>397.0</td>\n",
       "      <td>5258.0</td>\n",
       "      <td>26.504759</td>\n",
       "      <td>0.773142</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.143431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>415.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>12.031852</td>\n",
       "      <td>0.707582</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3.425802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>416.0</td>\n",
       "      <td>4176.0</td>\n",
       "      <td>20.096772</td>\n",
       "      <td>0.835974</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3.488614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    no_of_nodes  no_of_edges  average_degree  average_clustering_coefficient  \\\n",
       "0          26.0         81.0        6.238131                        0.703321   \n",
       "1          29.0         76.0        5.260353                        0.633320   \n",
       "2          32.0         85.0        5.349207                        0.683539   \n",
       "3          34.0        111.0        6.456137                        0.732987   \n",
       "4          36.0        156.0        8.758000                        0.713908   \n",
       "..          ...          ...             ...                             ...   \n",
       "95        376.0       6951.0       36.987107                        0.766860   \n",
       "96        380.0       5278.0       27.781883                        0.768408   \n",
       "97        397.0       5258.0       26.504759                        0.773142   \n",
       "98        415.0       2500.0       12.031852                        0.707582   \n",
       "99        416.0       4176.0       20.096772                        0.835974   \n",
       "\n",
       "    diameter  radius  powerlaw_exponent  \n",
       "0          3       2           2.407586  \n",
       "1          4       2           2.984418  \n",
       "2          3       2           2.365894  \n",
       "3          4       2           2.362633  \n",
       "4          7       4           3.267557  \n",
       "..       ...     ...                ...  \n",
       "95         5       3           2.778269  \n",
       "96         4       2           2.982870  \n",
       "97         4       2           2.143431  \n",
       "98         4       2           3.425802  \n",
       "99         5       3           3.488614  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_synthetic=df_synthetic.sort_values(by='no_of_nodes').reset_index(drop=True)\n",
    "df_synthetic=df_synthetic[['no_of_nodes','no_of_edges','average_degree','average_clustering_coefficient','diameter','radius','powerlaw_exponent']]\n",
    "#df_synthetic.to_csv('scripts/data/Network_Metrics_synthetic_dataset2.csv')\n",
    "df_synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7BKaEF7eQdP"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evipIrKx0OzF"
   },
   "source": [
    "# Synthetic Topology Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XItxQSZ0OzF"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "io0aFE2aMeKs"
   },
   "source": [
    "#### small_sized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjGmuYWHO8aX"
   },
   "outputs": [],
   "source": [
    "def small_sized():\n",
    "\n",
    "  degreedist = random.choice(['HD','Norm'])\n",
    "  if degreedist=='HD':\n",
    "    high_hub=True\n",
    "    Norm_hub=False\n",
    "  else:\n",
    "    high_hub=False\n",
    "    Norm_hub=True\n",
    "\n",
    "  if high_hub:\n",
    "\n",
    "    while True:\n",
    "      # random pick network to generate from synthetic dataset\n",
    "      our_choice = random.choice(df2.index)\n",
    "      extreme_range = True\n",
    "\n",
    "      interval=df2.no_of_nodes[our_choice]*0.3\n",
    "      low=df2.no_of_nodes[our_choice]-interval\n",
    "      high=df2.no_of_nodes[our_choice]\n",
    "\n",
    "      #get similar degree distribution\n",
    "      df_degreedistribution=df[(low<=df.no_of_nodes) & (df.no_of_nodes<=high)]           \n",
    "\n",
    "      #get similar average degree \n",
    "      index_degree=[]\n",
    "      our_degree = df2.average_degree[our_choice]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (our_degree-2 > df_degreedistribution['average_degree'][index]) | ( df_degreedistribution['average_degree'][index] > our_degree+2):\n",
    "          index_degree.append(index)\n",
    "      df_degreedistribution = df_degreedistribution.drop(index_degree)\n",
    "\n",
    "      #filter based on high difference of max degree threshold\n",
    "      index_degree2=[]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (df_degreedistribution.no_of_nodes[index]-max(df_degreedistribution.degree_of_nodes[index]))/df_degreedistribution.no_of_nodes[index]>0.2:\n",
    "          index_degree2.append(index)\n",
    "      if len(index_degree2)>0:\n",
    "        break\n",
    "\n",
    "    all_degrees=[]\n",
    "    for indexx in index_degree2:\n",
    "      for degreee in df_degreedistribution.degree_of_nodes[indexx]:\n",
    "        all_degrees.append(degreee)\n",
    "\n",
    "    #define measurement based networks\n",
    "    change_range = True\n",
    "    edge_corners=df2.no_of_edges[our_choice]*2\n",
    "    syn_nodes=int(df2.no_of_nodes[our_choice])\n",
    "    cluster_coef = round(df2.average_clustering_coefficient[our_choice],2)\n",
    "    dia = df2.diameter[our_choice]\n",
    "\n",
    "    threshold=0\n",
    "    while True:\n",
    "      threshold+=1\n",
    "      if threshold>=100000:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      #constraint on average degree\n",
    "      if edge_corners-10<=sum(nodes_degree_list)<=edge_corners+10:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          #generate havel hakimi network from sampled degree distribution\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>5:\n",
    "            continue\n",
    "          else:\n",
    "            #reducing components\n",
    "            for f in range(len(components)-1):\n",
    "              for target in list(components[f+1]):\n",
    "                source = random.choice(list(components[0])[:5])\n",
    "                g.add_edge(source, target)     \n",
    "\n",
    "            G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "            #constraint on average clustering coefficient\n",
    "            if cluster_coef-0.01<=round((nx.average_clustering(G)),2)<=cluster_coef+0.01:\n",
    "              if nx.diameter(G)<=dia:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue \n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "    if extreme_range:\n",
    "      \n",
    "      interval=df2.no_of_nodes[our_choice]*0.3\n",
    "      low=df2.no_of_nodes[our_choice]-interval\n",
    "      high=df2.no_of_nodes[our_choice]\n",
    "\n",
    "      #get similar degree distribution\n",
    "      df_degreedistribution=df[(low<=df.no_of_nodes) & (df.no_of_nodes<=high)]           \n",
    "\n",
    "      #get similar average degree \n",
    "      index_degree=[]\n",
    "      our_degree = df2.average_degree[our_choice]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (our_degree-2 > df_degreedistribution['average_degree'][index]) | ( df_degreedistribution['average_degree'][index] > our_degree+2):\n",
    "          index_degree.append(index)\n",
    "      df_degreedistribution = df_degreedistribution.drop(index_degree)\n",
    "\n",
    "      #filter based on high difference of max degree threshold\n",
    "      index_degree2=[]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (df_degreedistribution.no_of_nodes[index]-max(df_degreedistribution.degree_of_nodes[index]))/df_degreedistribution.no_of_nodes[index]>0.1:\n",
    "          index_degree2.append(index)\n",
    "\n",
    "      all_degrees=[]\n",
    "      for indexx in index_degree2:\n",
    "        for degreee in df_degreedistribution.degree_of_nodes[indexx]:\n",
    "          all_degrees.append(degreee)\n",
    "\n",
    "      #define measurement based networks\n",
    "      edge_corners=df2.no_of_edges[our_choice]*2\n",
    "      syn_nodes=int(df2.no_of_nodes[our_choice])\n",
    "      cluster_coef = round(df2.average_clustering_coefficient[our_choice],2)\n",
    "      dia = df2.diameter[our_choice]\n",
    "\n",
    "      threshold=0\n",
    "      while extreme_range:\n",
    "        threshold+=1\n",
    "        if threshold>=100000:\n",
    "          Norm_hub=True\n",
    "          break\n",
    "        nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "        #constraint on average degree\n",
    "        if edge_corners-10<=sum(nodes_degree_list)<=edge_corners+10:\n",
    "          try:\n",
    "            sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "            #generate havel hakimi network from sampled degree distribution\n",
    "            g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "            components = dict(enumerate(nx.connected_components(g)))\n",
    "            if len(components)>8:\n",
    "              continue\n",
    "            else:\n",
    "              #reducing components\n",
    "              for f in range(len(components)-1):\n",
    "                for target in list(components[f+1]):\n",
    "                  source = random.choice(list(components[0])[:3])\n",
    "                  g.add_edge(source, target)     \n",
    "\n",
    "              G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "              #constraint on average clustering coefficient\n",
    "              if cluster_coef-0.1<=round((nx.average_clustering(G)),2)<=cluster_coef+0.1:\n",
    "                if nx.diameter(G)<=dia:\n",
    "                  change_range=False\n",
    "                  extreme_range=False\n",
    "                  break\n",
    "                else:\n",
    "                  continue \n",
    "              else:\n",
    "                continue\n",
    "          except:\n",
    "            continue      \n",
    "  \n",
    "  #if extreme_range:\n",
    "  #  Norm_hub=True \n",
    "  if Norm_hub:\n",
    " \n",
    "    while True:\n",
    "      # random pick network to generate from synthetic dataset\n",
    "      our_choice = random.choice(df2.index)\n",
    "      extreme_range = True\n",
    "\n",
    "      interval=df2.no_of_nodes[our_choice]*0.3\n",
    "      low=df2.no_of_nodes[our_choice]-interval\n",
    "      high=df2.no_of_nodes[our_choice]\n",
    "\n",
    "      #get similar degree distribution\n",
    "      df_degreedistribution=df[(low<=df.no_of_nodes) & (df.no_of_nodes<=high)]           \n",
    "\n",
    "      #get similar average degree \n",
    "      index_degree=[]\n",
    "      our_degree = df2.average_degree[our_choice]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (our_degree-2 > df_degreedistribution['average_degree'][index]) | ( df_degreedistribution['average_degree'][index] > our_degree+2):\n",
    "          index_degree.append(index)\n",
    "      df_degreedistribution = df_degreedistribution.drop(index_degree)\n",
    "\n",
    "      if len(df_degreedistribution.iloc[:,:4])>0:\n",
    "        break\n",
    "\n",
    "    all_degrees=[]\n",
    "    for indexx in df_degreedistribution.index:\n",
    "      for degreee in df_degreedistribution.degree_of_nodes[indexx]:\n",
    "        all_degrees.append(degreee)\n",
    "\n",
    "    #define measurement based networks\n",
    "    change_range = True\n",
    "    edge_corners=df2.no_of_edges[our_choice]*2\n",
    "    syn_nodes=int(df2.no_of_nodes[our_choice])\n",
    "    cluster_coef = round(df2.average_clustering_coefficient[our_choice],2)\n",
    "    dia = df2.diameter[our_choice]\n",
    "\n",
    "    threshold=0\n",
    "    while True:\n",
    "      threshold+=1\n",
    "      if threshold>=10000:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      #constraint on average degree\n",
    "      if edge_corners-10<=sum(nodes_degree_list)<=edge_corners+10:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          #generate havel hakimi network from sampled degree distribution\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>1:\n",
    "            continue\n",
    "          else:\n",
    "            G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "            #constraint on average clustering coefficient\n",
    "            if cluster_coef-0.01<=round((nx.average_clustering(G)),2)<=cluster_coef+0.01:\n",
    "              if (dia-1)<=nx.diameter(G)<=dia:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue \n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "    threshold=0\n",
    "    while change_range:\n",
    "      threshold+=1\n",
    "      if threshold>=10000:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      if edge_corners-30<=sum(nodes_degree_list)<=edge_corners+30:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True)\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>1:\n",
    "            continue\n",
    "          else:\n",
    "            G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "            if cluster_coef-0.02<=round((nx.average_clustering(G)),2)<=cluster_coef+0.02:\n",
    "              if (dia-1)<=nx.diameter(G)<=dia:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue\n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "    if extreme_range:\n",
    "      interval=df2.no_of_nodes[our_choice]*0.5\n",
    "      low=df2.no_of_nodes[our_choice]-interval\n",
    "      high=df2.no_of_nodes[our_choice]\n",
    "\n",
    "      df_degreedistribution=df[(low<=df.no_of_nodes) & (df.no_of_nodes<=high)]          \n",
    "\n",
    "      all_degrees=[]\n",
    "      for indexx in df_degreedistribution.index:\n",
    "        for degreee in df_degreedistribution.degree_of_nodes[indexx]:\n",
    "          all_degrees.append(degreee)\n",
    "\n",
    "    while extreme_range:\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      if edge_corners-40<=sum(nodes_degree_list)<=edge_corners+40:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>10:\n",
    "            continue\n",
    "          else:\n",
    "            #reducing components here\n",
    "            for f in range(len(components)-1):   \n",
    "              source = list(components[0])[0]\n",
    "              target = random.choice(list(components[f+1]))\n",
    "              g.add_edge(source, target)\n",
    "            G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "            if cluster_coef-0.3<=round((nx.average_clustering(G)),2)<=cluster_coef+0.3:\n",
    "              if nx.diameter(G)<=dia:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue\n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "  return G,df_degreedistribution,our_choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyHzWQQnV0IZ"
   },
   "source": [
    "#### medium_sized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdMwGpHBQTjI"
   },
   "outputs": [],
   "source": [
    "def medium_sized():\n",
    "\n",
    "  degreedist = random.choice(['HD','Norm'])\n",
    "  if degreedist=='HD':\n",
    "    high_hub=True\n",
    "    Norm_hub=False\n",
    "  else:\n",
    "    high_hub=False\n",
    "    Norm_hub=True\n",
    "\n",
    "  if high_hub:\n",
    "\n",
    "    while True:\n",
    "      # random pick network to generate from synthetic dataset\n",
    "      our_choice = random.choice(df2.index)\n",
    "      extreme_range = True\n",
    "\n",
    "      interval=df2.no_of_nodes[our_choice]*0.3\n",
    "      low=df2.no_of_nodes[our_choice]-interval\n",
    "      high=df2.no_of_nodes[our_choice]\n",
    "\n",
    "      #get similar degree distribution\n",
    "      df_degreedistribution=df[(low<=df.no_of_nodes) & (df.no_of_nodes<=high)]           \n",
    "\n",
    "      #get similar average degree \n",
    "      index_degree=[]\n",
    "      our_degree = df2.average_degree[our_choice]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (our_degree-2 > df_degreedistribution['average_degree'][index]) | ( df_degreedistribution['average_degree'][index] > our_degree+2):\n",
    "          index_degree.append(index)\n",
    "      df_degreedistribution = df_degreedistribution.drop(index_degree)\n",
    "\n",
    "      #filter based on high difference of max degree threshold\n",
    "      index_degree2=[]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (df_degreedistribution.no_of_nodes[index]-max(df_degreedistribution.degree_of_nodes[index]))/df_degreedistribution.no_of_nodes[index]>0.2:\n",
    "          index_degree2.append(index)\n",
    "      if len(index_degree2)>0:\n",
    "        break\n",
    "\n",
    "    all_degrees=[]\n",
    "    for indexx in index_degree2:\n",
    "      for degreee in df_degreedistribution.degree_of_nodes[indexx]:\n",
    "        all_degrees.append(degreee)\n",
    "\n",
    "    #define measurement based networks\n",
    "    change_range = True\n",
    "    edge_corners=df2.no_of_edges[our_choice]*2\n",
    "    syn_nodes=int(df2.no_of_nodes[our_choice])\n",
    "    cluster_coef = round(df2.average_clustering_coefficient[our_choice],2)\n",
    "    dia = df2.diameter[our_choice]\n",
    "\n",
    "    threshold=0\n",
    "    while True:\n",
    "      threshold+=1\n",
    "      if threshold>=100000:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      #constraint on average degree\n",
    "      if edge_corners-20<=sum(nodes_degree_list)<=edge_corners+20:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          #generate havel hakimi network from sampled degree distribution\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>10:\n",
    "            continue\n",
    "          else:\n",
    "            #reducing components\n",
    "            for f in range(len(components)-1):\n",
    "              for target in list(components[f+1]):\n",
    "                source = random.choice(list(components[0])[:5])\n",
    "                g.add_edge(source, target)     \n",
    "\n",
    "            G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "            #constraint on average clustering coefficient\n",
    "            if cluster_coef-0.01<=round((nx.average_clustering(G)),2)<=cluster_coef+0.01:\n",
    "              if nx.diameter(G)<=dia:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue \n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "    threshold=0\n",
    "    while change_range:\n",
    "      threshold+=1\n",
    "      if threshold>=10000:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      #constraint on average degree\n",
    "      if edge_corners-20<=sum(nodes_degree_list)<=edge_corners+20:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          #generate havel hakimi network from sampled degree distribution\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>20:\n",
    "            continue\n",
    "          else:\n",
    "            #reducing components\n",
    "            for f in range(len(components)-1):\n",
    "              for target in list(components[f+1]):\n",
    "                source = random.choice(list(components[0])[:5])\n",
    "                g.add_edge(source, target)     \n",
    "\n",
    "            G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "            #constraint on average clustering coefficient\n",
    "            if cluster_coef-0.1<=round((nx.average_clustering(G)),2)<=cluster_coef+0.1:\n",
    "              if nx.diameter(G)<=dia:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue \n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "    threshold=0\n",
    "    while extreme_range:\n",
    "      threshold+=1\n",
    "      if threshold>=100000:\n",
    "        Norm_hub=True\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      #constraint on average degree\n",
    "      if edge_corners-20<=sum(nodes_degree_list)<=edge_corners+20:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          #generate havel hakimi network from sampled degree distribution\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>20:\n",
    "            continue\n",
    "          else:\n",
    "            #reducing components\n",
    "            for f in range(len(components)-1):\n",
    "              for target in list(components[f+1]):\n",
    "                source = random.choice(list(components[0])[:5])\n",
    "                g.add_edge(source, target)     \n",
    "\n",
    "            G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "            #constraint on average clustering coefficient\n",
    "            if cluster_coef-0.2<=round((nx.average_clustering(G)),2)<=cluster_coef+0.2:\n",
    "              if nx.diameter(G)<=dia+1:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue \n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "  if Norm_hub:\n",
    "    while True:\n",
    "      # random pick network to generate from synthetic dataset  \n",
    "      our_choice = random.choice(df2.index)\n",
    "      extreme_range = True\n",
    "\n",
    "      interval=df2.no_of_nodes[our_choice]*0.3\n",
    "      low=df2.no_of_nodes[our_choice]-interval\n",
    "      high=df2.no_of_nodes[our_choice]\n",
    "\n",
    "      #get similar degree distribution\n",
    "      df_degreedistribution=df[(low<=df.no_of_nodes) & (df.no_of_nodes<=high)]          \n",
    "\n",
    "      #get similar average degree \n",
    "      index_degree=[]\n",
    "      our_degree = df2.average_degree[our_choice]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (our_degree-2 > df_degreedistribution['average_degree'][index]) | ( df_degreedistribution['average_degree'][index] > our_degree+2):\n",
    "          index_degree.append(index)\n",
    "      df_degreedistribution = df_degreedistribution.drop(index_degree)\n",
    "\n",
    "      if len(df_degreedistribution.iloc[:,:4])>0:\n",
    "        break\n",
    "\n",
    "    all_degrees=[]\n",
    "    for indexx in df_degreedistribution.index:\n",
    "      for degreee in df_degreedistribution.degree_of_nodes[indexx]:\n",
    "        all_degrees.append(degreee)\n",
    "  \n",
    "    #define measurement based networks\n",
    "    change_range = True\n",
    "    edge_corners=df2.no_of_edges[our_choice]*2\n",
    "    syn_nodes=int(df2.no_of_nodes[our_choice])\n",
    "    cluster_coef = round(df2.average_clustering_coefficient[our_choice],2)\n",
    "    dia = df2.diameter[our_choice]\n",
    "\n",
    "    threshold=0\n",
    "    while True:\n",
    "      threshold+=1\n",
    "      if threshold>=10000:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "        \n",
    "      #constraint on average degree\n",
    "      if edge_corners-20<=sum(nodes_degree_list)<=edge_corners+20:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          #generate havel hakimi network from sampled degree distribution\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>10:\n",
    "            continue\n",
    "          else:\n",
    "            #reduce components\n",
    "            for f in range(len(components)-1):   \n",
    "              source = list(components[0])[0]\n",
    "              target = random.choice(list(components[f+1]))\n",
    "              g.add_edge(source, target)\n",
    "\n",
    "            G=g \n",
    "            #constraint on average clustering coefficient\n",
    "            if cluster_coef-0.01<=round((nx.average_clustering(G)),2)<=cluster_coef+0.01:\n",
    "              if (dia-1)<=nx.diameter(G)<=dia:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue\n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "    threshold=0\n",
    "    while change_range:\n",
    "      threshold+=1\n",
    "      if threshold>=10000:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      if edge_corners-30<=sum(nodes_degree_list)<=edge_corners+30:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>10:\n",
    "            continue\n",
    "          else:\n",
    "            #reduce components\n",
    "            for f in range(len(components)-1):   \n",
    "              source = list(components[0])[0]\n",
    "              target = random.choice(list(components[f+1]))\n",
    "              g.add_edge(source, target)\n",
    "            G=g \n",
    "            if cluster_coef-0.1<=round((nx.average_clustering(G)),2)<=cluster_coef+0.1:\n",
    "              if nx.diameter(G)<=dia:  \n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue\n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "    if extreme_range:\n",
    "      \n",
    "      interval=df2.no_of_nodes[our_choice]*0.3\n",
    "      low=df2.no_of_nodes[our_choice]-interval\n",
    "      high=df2.no_of_nodes[our_choice]\n",
    "\n",
    "      df_degreedistribution=df[(low<=df.no_of_nodes) & (df.no_of_nodes<=high)]           \n",
    "\n",
    "      all_degrees=[]\n",
    "      for indexx in df_degreedistribution.index:\n",
    "        for degreee in df_degreedistribution.degree_of_nodes[indexx]:\n",
    "          all_degrees.append(degreee)\n",
    "\n",
    "    while extreme_range:\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      if edge_corners-50<=sum(nodes_degree_list)<=edge_corners+50:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>30:\n",
    "            continue\n",
    "          else:\n",
    "            #reduce components\n",
    "            for f in range(len(components)-1):   \n",
    "              source = list(components[0])[0]\n",
    "              target = random.choice(list(components[f+1]))\n",
    "              g.add_edge(source, target)\n",
    "            G=g \n",
    "            if cluster_coef-0.3<=round((nx.average_clustering(G)),2)<=cluster_coef+0.3:\n",
    "              if nx.diameter(G)<=dia+5:   \n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue\n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "  return G,df_degreedistribution,our_choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZGSIVS9IreH"
   },
   "source": [
    "#### large_sized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sooIR2eNQ01W"
   },
   "outputs": [],
   "source": [
    "def large_sized():\n",
    "\n",
    "  degreedist = np.random.choice(['HD','Norm'],p=[0.3,0.7])\n",
    "  if degreedist=='HD':\n",
    "    high_hub=True\n",
    "    Norm_hub=False\n",
    "  else:\n",
    "    high_hub=False\n",
    "    Norm_hub=True\n",
    "  \n",
    "  if high_hub:\n",
    "    while True:\n",
    "      # random pick network to generate from synthetic dataset\n",
    "      our_choice = random.choice(df2.index)\n",
    "      extreme_range = True\n",
    "\n",
    "      interval=df2.no_of_nodes[our_choice]*0.3\n",
    "      low=df2.no_of_nodes[our_choice]-interval\n",
    "      high=df2.no_of_nodes[our_choice]\n",
    "\n",
    "      #get similar degree distribution\n",
    "      df_degreedistribution=df[(low<=df.no_of_nodes) & (df.no_of_nodes<=high)]           \n",
    "\n",
    "      #get similar average degree \n",
    "      index_degree=[]\n",
    "      our_degree = df2.average_degree[our_choice]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (our_degree-2 > df_degreedistribution['average_degree'][index]) | ( df_degreedistribution['average_degree'][index] > our_degree+2):\n",
    "          index_degree.append(index)\n",
    "      df_degreedistribution = df_degreedistribution.drop(index_degree)\n",
    "\n",
    "      #filter based on high difference of max degree threshold\n",
    "      index_degree2=[]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (df_degreedistribution.no_of_nodes[index]-max(df_degreedistribution.degree_of_nodes[index]))/df_degreedistribution.no_of_nodes[index]>0.2:\n",
    "          index_degree2.append(index)\n",
    "      if len(index_degree2)>0:\n",
    "        break\n",
    "\n",
    "    all_degrees=[]\n",
    "    for indexx in index_degree2:\n",
    "      for degreee in df_degreedistribution.degree_of_nodes[indexx]:\n",
    "        all_degrees.append(degreee)\n",
    "\n",
    "    #define measurement based networks\n",
    "    change_range = True\n",
    "    edge_corners=df2.no_of_edges[our_choice]*2\n",
    "    syn_nodes=int(df2.no_of_nodes[our_choice])\n",
    "    cluster_coef = round(df2.average_clustering_coefficient[our_choice],2)\n",
    "    dia = df2.diameter[our_choice]\n",
    "\n",
    "    if syn_nodes>300:\n",
    "        thres1,thres2=10,1000\n",
    "    else:\n",
    "        thres1,thres2=10000,10000  \n",
    "    threshold=0\n",
    "    while True:\n",
    "      threshold+=1\n",
    "      if threshold>=thres1:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      #constraint on average degree\n",
    "      if edge_corners-syn_nodes<=sum(nodes_degree_list)<=edge_corners+syn_nodes:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          #generate havel hakimi network from sampled degree distribution\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>20:\n",
    "            continue\n",
    "          else:\n",
    "            #reducing components\n",
    "            for f in range(len(components)-1):\n",
    "              for target in list(components[f+1]):\n",
    "                source = random.choice(list(components[0])[:5])\n",
    "                g.add_edge(source, target)     \n",
    "\n",
    "            G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "            #constraint on average clustering coefficient\n",
    "            if cluster_coef-0.15<=round((nx.average_clustering(G)),2)<=cluster_coef+0.15:\n",
    "              if nx.diameter(G)<=dia:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue \n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "    threshold=0\n",
    "    while change_range:\n",
    "      threshold+=1\n",
    "      if threshold>=thres2:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      inter = 1.5*syn_nodes\n",
    "      #constraint on average degree\n",
    "      if edge_corners-inter<=sum(nodes_degree_list)<=edge_corners+inter:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          #generate havel hakimi network from sampled degree distribution\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>35:\n",
    "            continue\n",
    "          else:\n",
    "            #reducing components\n",
    "            for f in range(len(components)-1):\n",
    "              for target in list(components[f+1]):\n",
    "                source = random.choice(list(components[0])[:5])\n",
    "                g.add_edge(source, target)     \n",
    "\n",
    "            G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "            #constraint on average clustering coefficient\n",
    "            if cluster_coef-0.15<=round((nx.average_clustering(G)),2)<=cluster_coef+0.15:\n",
    "              if nx.diameter(G)<=dia:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue \n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "    threshold=0\n",
    "    while extreme_range:\n",
    "      threshold+=1\n",
    "      if threshold>=10000:\n",
    "        Norm_hub=True\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      inter = 1.5*syn_nodes\n",
    "      #constraint on average degree\n",
    "      if edge_corners-inter<=sum(nodes_degree_list)<=edge_corners+inter:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          #generate havel hakimi network from sampled degree distribution\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>50:\n",
    "            continue\n",
    "          else:\n",
    "            #reducing components\n",
    "            for f in range(len(components)-1):\n",
    "              for target in list(components[f+1]):\n",
    "                source = random.choice(list(components[0])[:5])\n",
    "                g.add_edge(source, target)     \n",
    "\n",
    "            G=[g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)][0]\n",
    "            #constraint on average clustering coefficient\n",
    "            if cluster_coef-0.2<=round((nx.average_clustering(G)),2)<=cluster_coef+0.2:\n",
    "              change_range=False\n",
    "              extreme_range=False\n",
    "              break\n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "  if Norm_hub:\n",
    "    while True:\n",
    "      # random pick network to generate from synthetic dataset\n",
    "      our_choice = random.choice(df2.index)\n",
    "         \n",
    "      extreme_range = True\n",
    "\n",
    "      interval=df2.no_of_nodes[our_choice]*0.3\n",
    "      low=df2.no_of_nodes[our_choice]-interval\n",
    "      high=df2.no_of_nodes[our_choice]\n",
    "\n",
    "      #get similar degree distribution\n",
    "      df_degreedistribution=df[(low<=df.no_of_nodes) & (df.no_of_nodes<=high)]           \n",
    "\n",
    "      #get similar average degree \n",
    "      index_degree=[]\n",
    "      our_degree = df2.average_degree[our_choice]\n",
    "      for index in df_degreedistribution.index:\n",
    "        if (our_degree-2 > df_degreedistribution['average_degree'][index]) | ( df_degreedistribution['average_degree'][index] > our_degree+2):\n",
    "          index_degree.append(index)\n",
    "      df_degreedistribution = df_degreedistribution.drop(index_degree)\n",
    "\n",
    "      if len(df_degreedistribution.iloc[:,:4])>0:\n",
    "        break\n",
    "\n",
    "    all_degrees=[]\n",
    "    for indexx in df_degreedistribution.index:\n",
    "      for degreee in df_degreedistribution.degree_of_nodes[indexx]:\n",
    "        all_degrees.append(degreee)\n",
    "\n",
    "    #define measurement based networks\n",
    "    change_range = True\n",
    "    edge_corners=df2.no_of_edges[our_choice]*2\n",
    "    syn_nodes=int(df2.no_of_nodes[our_choice])\n",
    "    cluster_coef = round(df2.average_clustering_coefficient[our_choice],2)\n",
    "    dia = df2.diameter[our_choice]\n",
    "\n",
    "    if syn_nodes>340:\n",
    "        thres1,thres2=1,1000\n",
    "    else:\n",
    "        thres1,thres2=1000,10000  \n",
    "    threshold=0\n",
    "    while True:\n",
    "      threshold+=1\n",
    "      if threshold>=thres1:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      #constraint on average degree\n",
    "      inter = syn_nodes\n",
    "      if edge_corners-inter<=sum(nodes_degree_list)<=edge_corners+inter:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          #generate havel hakimi network from sampled degree distribution\n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "\n",
    "          if len(components)>50:\n",
    "            continue\n",
    "          else:\n",
    "            #reduce components\n",
    "            for f in range(len(components)-1):   \n",
    "              source = list(components[0])[0]\n",
    "              target = random.choice(list(components[f+1]))\n",
    "              g.add_edge(source, target)\n",
    "            G=g \n",
    "            #constraint on average clustering coefficient\n",
    "            if cluster_coef-0.15<=round((nx.average_clustering(G)),2)<=cluster_coef+0.15:\n",
    "              if nx.diameter(G)<=dia:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue\n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "    threshold=0\n",
    "    while change_range:\n",
    "      threshold+=1\n",
    "      if threshold>=thres2:\n",
    "        break\n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      inter = 1.5*syn_nodes\n",
    "      if edge_corners-inter<=sum(nodes_degree_list)<=edge_corners+inter:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          \n",
    "          if len(components)>50:\n",
    "            continue\n",
    "          else:\n",
    "            #reduce components\n",
    "            for f in range(len(components)-1):   \n",
    "              source = list(components[0])[0]\n",
    "              target = random.choice(list(components[f+1]))\n",
    "              g.add_edge(source, target)\n",
    "\n",
    "            G=g \n",
    "            if cluster_coef-0.2<=round((nx.average_clustering(G)),2)<=cluster_coef+0.2:\n",
    "              if nx.diameter(G)<=dia+5:\n",
    "                change_range=False\n",
    "                extreme_range=False\n",
    "                break\n",
    "              else:\n",
    "                continue\n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "\n",
    "    threshold=0     \n",
    "    while extreme_range:\n",
    "      threshold+=1\n",
    "      if threshold>=10000:\n",
    "        break    \n",
    "      nodes_degree_list=choices(all_degrees,k=syn_nodes)\n",
    "      inter = 2*syn_nodes\n",
    "      if edge_corners-inter<=sum(nodes_degree_list)<=edge_corners+inter:\n",
    "        try:\n",
    "          sort_degree_list=sorted(nodes_degree_list,reverse=True) \n",
    "          g=nx.havel_hakimi_graph(sort_degree_list, create_using=None) \n",
    "          components = dict(enumerate(nx.connected_components(g)))\n",
    "          if len(components)>50:\n",
    "            continue\n",
    "          else:\n",
    "            #reduce components\n",
    "            for f in range(len(components)-1):   \n",
    "              source = list(components[0])[0]\n",
    "              target = random.choice(list(components[f+1]))\n",
    "              g.add_edge(source, target)       \n",
    "\n",
    "            G=g \n",
    "            if cluster_coef-0.3<=round((nx.average_clustering(G)),2)<=cluster_coef+0.3:\n",
    "              change_range=False\n",
    "              extreme_range=False\n",
    "              break\n",
    "            else:\n",
    "              continue\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "  return G,df_degreedistribution,our_choice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0NS_G5KR9Rm"
   },
   "source": [
    "#### generate network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYZlQSvuOZWd"
   },
   "outputs": [],
   "source": [
    "#load real dataset\n",
    "a_file = open(\"scripts/data/Network_Metrics_real_dataset.pkl\", \"rb\")\n",
    "data = pickle.load(a_file)\n",
    "data=data.drop(columns=['repo'])\n",
    "data=data.sort_values(by='no_of_nodes').reset_index(drop=True)\n",
    "df=data.iloc[:,:]\n",
    "\n",
    "# choose group from small_sized,medium_sized,large_sized\n",
    "choosen_group = random.choice(['small_sized','medium_sized','large_sized'])\n",
    "\n",
    "iterate_dic = {'small_sized':[0,16],'medium_sized':[16,43],'large_sized':[43,100]}\n",
    "aa=iterate_dic[choosen_group][0]\n",
    "bb=iterate_dic[choosen_group][1]\n",
    "\n",
    "#load synthetic dataset\n",
    "df2=pd.read_csv('scripts/data/Network_Metrics_synthetic_dataset.csv',index_col=0)\n",
    "df2=df2.sort_values(by='no_of_nodes').reset_index(drop=True)\n",
    "df2=df2.iloc[aa:bb,:].reset_index(drop=True)\n",
    "\n",
    "#generate topology as per size\n",
    "if choosen_group=='small_sized':\n",
    "  G,df_degreedistribution,our_choice=small_sized()\n",
    "elif choosen_group=='medium_sized':\n",
    "  G,df_degreedistribution,our_choice=medium_sized()\n",
    "elif choosen_group=='large_sized':\n",
    "  G,df_degreedistribution,our_choice=large_sized()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cliques prioritization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vvyS9ZZ0OzI"
   },
   "outputs": [],
   "source": [
    "df_model=pd.read_csv('scripts/data/usecase_data/df_model.csv',index_col=0)\n",
    "\n",
    "#model\n",
    "X=df_model.iloc[:,[0,1,2,3,7]]\n",
    "y=df_model.iloc[:,[9]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=2550)  \n",
    "\n",
    "model = Ridge()                  \n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(predictions, y_test)\n",
    "\n",
    "#identify cliques in the synthetic network\n",
    "from networkx import enumerate_all_cliques,find_cliques\n",
    "lst_cliques=list(find_cliques(G))\n",
    "\n",
    "#create dataframe of cliques\n",
    "df_generated = pd.DataFrame([lst_cliques]).T\n",
    "df_generated = df_generated.rename(columns={0:'committer'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvKkF-Tr0OzI"
   },
   "outputs": [],
   "source": [
    "#find predictor network centrality metrics\n",
    "df_generated['degree_sum']=0.0\n",
    "df_generated['betweenness_sum']=0.0\n",
    "df_generated['closeness_sum']=0.0\n",
    "df_generated['clustering_sum']=0.0\n",
    "df_generated['eigenvector_sum']=0.0\n",
    "df_generated['pagerank_sum']=0.0\n",
    "df_generated['hubs_sum']=0.0\n",
    "df_generated['Number of Hubs']=0\n",
    "\n",
    "hub_limit=int(sorted(dict(nx.degree(G)).values(),reverse=True)[0]*0.5)\n",
    "betweenness_dict=nx.betweenness_centrality(G)\n",
    "closeness_dict=nx.closeness_centrality(G)\n",
    "hubs_dict=nx.hits(G)[0]\n",
    "pagerank_dict=nx.pagerank(G)\n",
    "eigenvector_dict=nx.eigenvector_centrality(G)\n",
    "clustering_dict=nx.clustering(G)\n",
    "\n",
    "for index in df_generated.index:\n",
    "    \n",
    "    clustering_sum=0.0\n",
    "    eigenvector_sum=0.0\n",
    "    pagerank_sum=0.0\n",
    "    hubs_sum=0.0\n",
    "    count_hubs=0 \n",
    "    degree_sum=0.0\n",
    "    betweenness_sum=0.0\n",
    "    closeness_sum=0.0\n",
    "    for author in df_generated['committer'][index]:\n",
    "        try:\n",
    "            degree_sum+=G.degree(author)\n",
    "            betweenness_sum+=betweenness_dict[author]\n",
    "            closeness_sum+=closeness_dict[author] \n",
    "            clustering_sum+=clustering_dict[author]\n",
    "            eigenvector_sum+=eigenvector_dict[author]\n",
    "            pagerank_sum+=pagerank_dict[author]\n",
    "            hubs_sum+=hubs_dict[author]\n",
    "            if G.degree(author)>hub_limit:\n",
    "                count_hubs+=1\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    # take average of all centrality measures\n",
    "    df_generated['degree_sum'][index]=degree_sum/len(df_generated['committer'][index])\n",
    "    df_generated['betweenness_sum'][index]=betweenness_sum/len(df_generated['committer'][index])\n",
    "    df_generated['closeness_sum'][index]=closeness_sum/len(df_generated['committer'][index])\n",
    "    df_generated['clustering_sum'][index]=clustering_sum/len(df_generated['committer'][index])\n",
    "    df_generated['eigenvector_sum'][index]=eigenvector_sum/len(df_generated['committer'][index])\n",
    "    df_generated['pagerank_sum'][index]=pagerank_sum/len(df_generated['committer'][index])\n",
    "    df_generated['hubs_sum'][index]=hubs_sum/len(df_generated['committer'][index])\n",
    "    df_generated['Number of Hubs'][index]=count_hubs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJw75HeT0OzJ"
   },
   "outputs": [],
   "source": [
    "df_generated['Developers']=df_generated['committer'].apply(lambda x: len(x))\n",
    "\n",
    "df_generated_copy=df_generated.copy()\n",
    "#df_generated.to_csv('scripts/data/usecase_data/df_generated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2JJF5aq0OzJ"
   },
   "outputs": [],
   "source": [
    "#non average centrality measures\n",
    "df_generated=pd.read_csv('scripts/data/usecase_data/df_generated.csv',index_col=0)\n",
    "data_lst = [df_generated]\n",
    "for data in data_lst:\n",
    "    data['degree_sum']=data['degree_sum']*data['Developers']\n",
    "    data['betweenness_sum']=data['betweenness_sum']*data['Developers']\n",
    "    data['closeness_sum']=data['closeness_sum']*data['Developers']\n",
    "    data['clustering_sum']=data['clustering_sum']*data['Developers']\n",
    "    data['eigenvector_sum']=data['eigenvector_sum']*data['Developers']\n",
    "    data['pagerank_sum']=data['pagerank_sum']*data['Developers']\n",
    "    data['hubs_sum']=data['hubs_sum']*data['Developers']\n",
    "\n",
    "#normalize predictor metrics\n",
    "cols_to_norm = ['degree_sum','betweenness_sum','closeness_sum','Developers','clustering_sum', 'eigenvector_sum', \n",
    "                'pagerank_sum','hubs_sum','Number of Hubs']\n",
    "data_lst = [df_generated]\n",
    "for data in data_lst:\n",
    "    data[cols_to_norm] = data[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lar56vTn0OzJ"
   },
   "outputs": [],
   "source": [
    "#log transformation\n",
    "df_generated=df_generated[['degree_sum','betweenness_sum','closeness_sum','Developers','hubs_sum']]\n",
    "df_generated=df_generated+0.000001 \n",
    "\n",
    "df_generated['degree_sum'] = np.log(df_generated['degree_sum'])\n",
    "df_generated['Developers'] = np.log(df_generated['Developers'])\n",
    "df_generated['closeness_sum'] = np.log(df_generated['closeness_sum'])\n",
    "df_generated['betweenness_sum'] = np.log(df_generated['betweenness_sum'])\n",
    "df_generated['hubs_sum'] = np.log(df_generated['hubs_sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-Tnk0ov0OzJ"
   },
   "outputs": [],
   "source": [
    "#predict bug probability\n",
    "df_generated['bug_probability']=model.predict(df_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AoqcEbyk0OzJ"
   },
   "outputs": [],
   "source": [
    "# reverse log transform\n",
    "df_generated.iloc[:,[0,1,2,3,4,5]] = np.exp(df_generated.iloc[:,[0,1,2,3,4,5]])\n",
    "df_generated['committer']=df_generated_copy['committer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generated=df_generated.rename(columns={'committer':'Cliques'})\n",
    "df_generated=df_generated[['Cliques','degree_sum','betweenness_sum','closeness_sum','Developers','hubs_sum','bug_probability']]\n",
    "sort_cliques=df_generated.sort_values(by='bug_probability',ascending=False)\n",
    "#sort_cliques.to_csv('scripts/data/usecase_data/sort_cliques.csv')\n",
    "#display(HTML(sort_cliques.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57vvCYOq0OzK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxnfAJjZ0OzK"
   },
   "source": [
    "##### END"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "KhkOQfndEVhd",
    "knf5NgNnSVV8",
    "zfLDbcOESd_6",
    "6ttnkJrBTWBA",
    "At_JxmJfXXH8",
    "1IozSu_vKiFB",
    "evipIrKx0OzF",
    "io0aFE2aMeKs",
    "JyHzWQQnV0IZ",
    "gZGSIVS9IreH",
    "GKutKuvl0OzI"
   ],
   "name": "Network_Generator_Main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
